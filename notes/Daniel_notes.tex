\documentclass{article}
\usepackage[utf8]{inputenc}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}


% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{listings}
\usepackage{xcolor}

% Setup for Python code listings
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!60!black},
  showstringspaces=false,
  frame=single,
  columns=fullflexible
}

% Useful packages
\usepackage{color,tikz}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{caption,subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{varwidth}
%\def\Item$#1${\item $\displaystyle#1$
%   \hfill\refstepcounter{equation}(\theequation)}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{euscript}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[titletoc,title]{appendix}
\usepackage{upgreek}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{bbm}

\makeatletter
%\newcommand{\@giventhatstar}[2]{\left(#1\,\middle|\,#2\right)}
\newcommand{\@giventhatnostar}[3][]{#1(#2\,#1|\,#3#1)}
\newcommand{\giventhat}{\@ifstar\@giventhatstar\@giventhatnostar}
\makeatother

\renewcommand{\rmdefault}{ppl}

\usepackage{float}
\usepackage[scaled]{helvet}
\usepackage[utf8]{inputenc}
% \usepackage{algorithm}
% \usepackage[noend]{algorithmic}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \SetKwInput{KwInput}{Input}                % Set the Input
% \SetKwInput{KwOutput}{Output}     
% \SetKwInput{KwInit}{Initialization}     
\usepackage{algorithm,algpseudocode}% http://ctan.org/pkg/{algorithms,algorithmx}
%\algnewcommand{\Inputs}[1]{%
%  \State \textbf{Inputs:}
%  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
%}
%\algnewcommand{\Initialize}[1]{%
%  \State \textbf{Initialize:}
%  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
%}
\usepackage{natbib}
\usepackage{tcolorbox}
\newcommand{\brbox}{\begin{tcolorbox}[colback=blue!10!white,colframe=orange!50!black,title=Remark]}
\newcommand{\bibox}{\begin{tcolorbox}[colback=orange!10!white,colframe=blue!50!black,title=Idea]}
\newcommand{\bpbox}{\begin{tcolorbox}[colback=red!10!white,colframe=green!50!black,title=Problem]}
\newcommand{\ebox}{\end{tcolorbox}}

\definecolor{light-gray}{gray}{0.6} 
%\usepackage[color=red, fontsize=55pt, stamp=true, text=IN~PROGRESS]{draftwatermark}

%\textwidth 15.9cm \textheight 22.1cm \evensidemargin 0cm
%\oddsidemargin 0cm \topmargin 0cm
%\setlength{\parindent}{0pt}
%\renewcommand{\baselinestretch}{1.15}
%\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\vb}{\vspace{3mm}}



\newcommand{\DD}{{\rm d}}
\newcommand{\s}{^\star}
% \newcommand{\PP}{{\mathbb P}}
% \newcommand{\EE}{{\mathbb E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\xx}{\mathbb{X}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\pp}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ee}{\mathbb{E}}
\newcommand{\vt}{\vartheta}
\newcommand{\hN}{^{(N)}}
\newcommand{\rmd}{\mathrm{d}}


\newcommand{\scA}{\mathscr{A}}
\newcommand{\scB}{\mathscr{B}}
\newcommand{\scC}{\mathscr{C}}
\newcommand{\scD}{\mathscr{D}}
\newcommand{\scF}{\mathscr{F}}
\newcommand{\scG}{\mathscr{G}}
\newcommand{\scH}{\mathscr{H}}
\newcommand{\scI}{\mathscr{I}}
\newcommand{\scJ}{\mathscr{J}}
\newcommand{\scK}{\mathscr{K}}
\newcommand{\scL}{\mathscr{L}}
\newcommand{\scM}{\mathscr{M}}
\newcommand{\scP}{\mathscr{P}}
\newcommand{\scS}{\mathscr{S}}
\newcommand{\scR}{\mathscr{R}}
\newcommand{\scU}{\mathscr{U}}
\newcommand{\scE}{\mathscr{E}}
\newcommand{\scT}{\mathscr{T}}
\newcommand{\scV}{\mathscr{V}}
\newcommand{\scW}{\mathscr{W}}
\newcommand{\scX}{\mathscr{X}}
\newcommand{\scY}{\mathscr{Y}}
\newcommand{\scZ}{\mathscr{Z}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\bQ}{Q}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\intg}{\mathbb{Z}}
\newcommand{\natr}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\zlow}{z_{\sf low}}
\newcommand{\zup}{z_{\sf up}}
\newcommand{\elow}{E_{\sf low}}
\newcommand{\eup}{E_{\sf up}}

\newcommand{\mc}{\mathcal}
\newcommand{\m}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mbb}{\mathbb}
\newcommand{\mscr}{\mathscr}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\wb}{\overline}
\newcommand{\var}{\mathbb{V}ar}
\newcommand{\Vol}{\mathsf{Vol}}
\newcommand{\ebic}{\mathsf{EBIC}}

\renewcommand{\v}[1]{\boldsymbol{#1}}

\newcommand{\tr}{\mathsf{Tr}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\tht}{\boldsymbol{\theta}}
\newcommand{\hbeta}{\widehat \bbeta}
\newcommand{\tbeta}{\widetilde \bbeta}

\newcommand{\red}{\color{red}}
\newcommand{\hl}{\textcolor{blue}}
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\minimize}{min\,}
\DeclareMathOperator*{\maximize}{max\,}
\DeclareMathOperator*{\argmin}{argmin\,}
\DeclareMathOperator*{\subjectto}{subject\,\, to\,}
\DeclareMathOperator*{\diag}{Diag\,}

%%%%%%%%%%%%%
% Begin Sarat's definitions
%%%%%%%%%%%%%%%
\def\boxit#1{\vbox{\hrule\hbox{\vrule\kern6pt
          \vbox{\kern6pt#1\kern6pt}\kern6pt\vrule}\hrule}}
\def\sbmcomment#1{\vskip 2mm\boxit{\vskip -2mm{\color{cyan}\bf#1} {\color{cyan}\bf -- SBM\vskip 2mm}}\vskip 2mm}
\newcommand{\sbmc}[1]{{\color{cyan}#1}}
%%%%%%%%%%%%%
% End Sarat's definitions
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%
% Begin Matias's definitions
%%%%%%%%%%%%%%%
\def\boxit#1{\vbox{\hrule\hbox{\vrule\kern6pt
          \vbox{\kern6pt#1\kern6pt}\kern6pt\vrule}\hrule}}
\def\mqcomment#1{\vskip 2mm\boxit{\vskip -2mm{\color{red}\bf#1} {\color{red}\bf -- MQ\vskip 2mm}}\vskip 2mm}
\newcommand{\mqc}[1]{{\color{red}#1}}
%%%%%%%%%%%%%
% End Matias's definitions
%%%%%%%%%%%%%%%


\newtheorem{theorem}{Theorem}
\newtheorem{algorithm1}{Algorithm}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\title{Notes by Daniel}
% in the Artificial Intelligence Era
\author{}

\begin{document}

\maketitle


\section{Core Problem Formulation}
Consider a symmetric positive semi-definite matrix $A \in \reals^{p \times p}$ and a vector $\v b \in \reals^{p}$. For each binary vector $\v s \in \{0, 1\}^p$, let $A_{[\v s]}$ denote the principal submatrix of $A$ indexed by $\{j: s_j = 1\}$, and $\v b_{[\v s]}$ as the subvector of $\v b$ on the same index set. 
For a given positive integer $k \leq p$ (usually $k$ is much smaller than $p$), our aim is solve the sparse constrained problem:
\begin{align}
    \maximize_{\v s \in \{0, 1\}^p} \v b_{[\v s]}^\top (A_{[\v s]})^\dag \v b_{[\v s]}, \quad \subjectto\,\, \v |\v s| \leq k.
    \label{eqn:opt-master}
\end{align}
where $|\v s|$ denotes the number of ones in $\v s$ and $(A_{[\v s]})^\dag$ denotes the Moore-Penrose pseudo-inverse of $A_{[\v s]}$. It is important to keep in mind that $(A_{[\v s]})^\dag$ is not a submatrix of $A^\dag$. Note that if $A$ is full-rank (i.e., positive definite), each $A_{[\v s]}$ is invertible. Also note that each $A_{[\v s]}$ is (symmetric) positive semi-definite since it is a principal submatrix of $A$, which is assumed to be symmetric positive semi-definite. The problem \eqref{eqn:opt-master} is NP-hard and serves as a unifying framework for several important problems in statistics and machine learning. Below we detail how various domain-specific problems can be reformulated as instances of this core problem. We now list some of these problems.

\begin{example}({\bf Minimum-Variance Portfolio Selection:})
\normalfont
The classical minimum-variance portfolio optimization typically produces dense solutions with nonzero weights across all assets, which can lead to high transaction costs and increased estimation error. To address these limitations, sparse portfolio selection incorporates an explicit cardinality constraint that limits the number of assets to at most \(k\). This yields the optimization problem gievn by
\begin{align}
    \minimize_{\v \beta \in \reals^p} \v \beta^\top \Sigma \v \beta, \quad \subjectto \v 1^\top \v \beta = 1, \,\, \|\v \beta\|_0 \leq k,
    \label{eqn:sparse-portfolio}
\end{align}
where \(\Sigma \in \reals^{p \times p}\) is an invertible covariance matrix of asset returns.
This formulation can be equivalently expressed as a binary-constrained optimization problem:
\begin{align}
    \minimize_{\substack{\v s \in \{0, 1\}^p \\ |\v s| \leq k}} \minimize_{\substack{\v \beta_{[\v s]} \in \reals^{|\v s|} \\ \v 1^\top \v \beta_{[\v s]} = 1}} \v \beta_{[\v s]}^\top \Sigma_{[\v s]} \v \beta_{[\v s]}. 
    \label{eqn:sparse-portfolio-binary}
\end{align}
For any fixed support \(\v s\), the inner minimization admits a closed-form solution,  given by
\[
\v \beta_{[\v s]}^* = \frac{(\Sigma_{[\v s]})^{-1} \v 1}{\v 1^\top (\Sigma_{[\v s]})^{-1} \v 1},
\]
yielding the minimal portfolio variance 
\(
\nicefrac{1}{\v 1^\top (\Sigma_{[\v s]})^{-1} \v 1}.
\)
Thus, the combinatorial optimization problem reduces to
\begin{align}
    \maximize_{\v s \in \{0, 1\}^p} \v 1^\top (\Sigma_{[\v s]})^{-1} \v 1, \quad \subjectto |\v s| \leq k.
    \label{eqn:sparse-portfolio-master}
\end{align}

This precisely matches the form of the core problem \eqref{eqn:opt-master} with \(A = \Sigma\) and \(\v b = \v 1\), demonstrating that sparse portfolio selection is another important instance of the core problem.
\end{example}

\noindent
{\bf A Crucial Observation:}
Recall the core problem \eqref{eqn:opt-master}. Assume that $A$ has a full rank (i.e., positive definite) and all the elements of $\v b$ are non-zero. Then, for every $\v s\in \{0, 1\}^p$,
\begin{align*}
    \v b_{[\v s]}^\top (A_{[\v s]})^\dag \v b_{[\v s]} 
    &=  \v b_{[\v s]}^\top (A_{[\v s]})^{-1} \v b_{[\v s]}\\
    &=  \v b_{[\v s]}^\top (A_{[\v s]})^{-1} \v b_{[\v s]}\\
    &=  \v 1^\top \diag(\v b_{[\v s]}) (A_{[\v s]})^{-1} \diag(\v b_{[\v s]}) \v 1\\
    &=  \v 1^\top \lt(\diag(\v 1/\v b_{[\v s]}) A_{[\v s]} \diag(\v 1/\v b_{[\v s]}) \rt)^{-1} \v 1\\
    &=  \v 1^\top \lt(\Sigma_{[\v s]}\rt)^{-1} \v 1,
\end{align*}
where $\Sigma = \diag(\v 1/\v b) A \diag(\v 1/\v b)$. This implies that, under the minor assumption, solving the minimum-variance portfolio problem is equivalent to solving the core problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
\normalfont
{\bf Column Subset Selection Problem:}
Let \(X \in \reals^{m \times p}\) be a data matrix and define \(X_{[:,\v s]}\) as the submatrix with the selected columns associated with ones in $\v s$. Further define the (orthogonal) projector
\[
P_{\v s} := X_{[:,\v s]}\bigl(X_{[:,\v s]}^\top X_{[:,\v s]}\bigr)^{\dagger} X_{[:,\v s]}^\top.
\]
The \emph{column subset selection problem (CSSP)} in Frobenius norm is
\begin{align}
\min_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;\bigl\|\,X - P_{\v s} X\,\bigr\|_F^2.
\label{eq:cssp-frob}
\end{align}
Using \(\|B\|_F^2 = \tr(B^\top B)\) and the idempotence of \(P_{\v s}\), we obtain 
\begin{align*}
%\min_{\v s \in \{0, 1\}^p:\,|S|\le k} \;
\bigl\|X - P_{\v s} X\bigr\|_F^2
= \|X\|_F^2 - 
%\max_{\v s \in \{0, 1\}^p:\,|S|\le k} 
\tr\!\bigl(X^\top P_{\v s} X\bigr),
\end{align*}
which implies, \eqref{eq:cssp-frob} is equivalent to
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \tr\!\bigl(X^\top P_{\v s} X\bigr).
\label{eq:cssp-trace}
\end{align}
Introduce a Rademacher vector \(\v \xi \in \{\pm1\}^n\) with \(\ee[\v \xi \v \xi^\top]=I\) (i.e, generate elements of \(\v \xi\) independently and uniformly on $\{-1, +1\}$ ).
By Hutchinson's identity \cite{hutchinson1989stochastic}, 
\(\tr(B)=\ee_{\v \xi}[\v \xi^\top B \v \xi]\) for any square \(B\),
so \eqref{eq:cssp-trace} can be written as
\begin{align}
\tr\!\bigl(X^\top P_{\v s} X\bigr) = \ee_{\v \xi}\!\Big[\,
\v \xi^\top X^\top P_{\v s} X \v \xi
\,\Big].
\label{eq:cssp-hutch}
\end{align}
Now set
\[
A := X^\top X \in \reals^{n\times n},
\qquad
\v b^{(\v \xi)} := X^\top X\,\v \xi \in \reals^{n},
\]
so that \(A_{[\v s]} = X_{[:,\v s]}^\top X_{[:,\v s]}\) and
\(
\v b^{(\v \xi)}_{[\v s]} = X_{[:,\v s]}^\top X \xi.
\)
Then the inner quadratic in \eqref{eq:cssp-hutch} becomes
\[
\v \xi^\top X^\top P_{\v s} X \v \xi
= \bigl(\v b^{(\v \xi)}_{[\v s]}\bigr)^\top \bigl(A_{[\v s]}\bigr)^{\dagger}\, \v b^{(\v \xi)}_{[\v s]},
\]
and the CSSP objective can be expressed as the following expected instance of our core problem:
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
\ee_{\v \xi}\!\Big[\,
\bigl(\v b^{(\v \xi)}_{[\v s]}\bigr)^\top\bigl(A_{[\v s]}\bigr)^{\dagger}\, \v b^{(\v \xi)}_{[\v s]}
\,\Big].
\label{eq:cssp-core-expected}
\end{align}
\end{example}

{\red Daniel will work on column subset selection problem focusing on:
\begin{itemize}
    \item Rewrite the following results,
    \item Rewrite the algorithm,
    \item Investigate improvements techniques,
    \item Running simulations to compare with existing methods.
\end{itemize}}

\section{CSSP - Daniel}
\subsection{Boolean Relaxation}
We make the following key assumption.
\noindent 

{\bf Assumption 1:}
\textit{Matrix $A = X^{\top}X$ is positive definite}.

This assumption is not restricitive, because we can perform a 
ridge regularization, by replacing $A$ with $A + \lambda I$ for some $\lambda > 0$ (Moka et al., 2025). ( Also cited (Fastrich, Paterlini and Winker, 2015))

Now, we show that all elements of $\v b^{(\v \xi)} = X^\top X \v\xi = A \v\xi$ is non-zero.
\begin{align}
    \v b^{(\v \xi)} &= X^\top X \v\xi = A \v\xi \\
    \iff A^{-1} \v\b^{\v \xi}
\end{align}
Note that, since $A$ is postive definite, 
Since $\v \xi \in \{ \pm 1 \}^n$, 
all elements of $\v b^{(\v \xi)} = X^\top X \v\xi$ is non-zero.

This is because, since $A = X^\top X \succ 0$

We can reformulate \eqref{eq:cssp-core-expected} into 
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \ee_{\v \xi} \! \Big[\ \v 1^\top (\Sigma^{(\v \xi)}_{[\v s]})^{-1} \v 1 \Big],
\end{align}
where \[
\Sigma^{(\v \xi)}_{[\v s]} := \diag(\v 1 / \v b^{(\v \xi)}_{[\v s]}) \bigl(A_{[\v s]}\bigr) \diag(\v 1 / \v b^{(\v \xi)}_{[\v s]})
\]
assuming that $A$ is a full rank, using the "crucial obsservation" from earlier.

\textit{We might not be able to make the full-rank assumption, and also have to prove that b is all non-zero}


\textit{Also, clearly state what $f_\delta(t)$ is.}

Now we expect to prove that Boolean relaxation of \eqref{eqn:sparse-portfolio-master} is is indeed a boolean relaxation of \eqref{eq:cssp-core-expected} thanks to the fact that expectation operator is a linear operator.

\section{Boolean relaxation}
We now provide a Boolean relaxation of \eqref{eqn:sparse-portfolio-master} as an auxiliary continuous function on $[0, 1]^p$, controlled by a tuning parameter $\delta > 0$.  \sbmc{[This stuff is from Moka et al (2025) on portfolio optimization]}.
To simplify the notation, define 
\begin{align}
T_{\v t} = \diag(\v t) \quad 
\text{and} \quad 
\wt \Sigma_{\v t} = T_{\v t}\Sigma T_{\v t} + \delta (I - T_{\v t}^2).
\label{eqn:defn-Sigma}
\end{align}
Then, our proposed Boolean relaxation of \eqref{eqn:sparse-portfolio-master} is given by
\begin{align}
    \label{eqn:opt-br2}
    \minimize_{\v t \in \cC_k} f_\delta(\v t), \quad \text{where}\,\,\,\,
f_\delta(\v t) = - \v t^\top {\wt \Sigma}_{\v t}^{-1} \v t,
\end{align}
and for each $k$ the constraint set $\cC_k$ is a polytope defined as
\begin{align}
\cC_k = \{ \v t \in [0, 1]^p : \v t^\top \v 1 \leq k\}.
\label{eqn:defn-polytope}
\end{align} 

The following result, Theorem~\ref{thm:relaxation-properties}, shows why \eqref{eqn:opt-br2} is a relaxation of the target problem \eqref{eqn:opt-bc3}. It shows that $f_\delta(\v t)$ is continuous on the hypercube $[0, 1]^p$ and its shape can be controlled by the auxiliary parameter $\delta$ while keeping the values of $f_\delta(\v t)$ fixed---independent of $\delta$---at all the (binary) corners $\v s \in \{ 0,1\}^p$. In addition, (iii) shows that $f_\delta(\v t)$ increases with $\delta$ for any fixed interior point $\v t$, while (iv) shows that the optimum of \eqref{eqn:opt-br2} is on a simplex.
\begin{theorem}[Theorems 2 \& 3 of \cite{moka2025scalable}]
    \label{thm:relaxation-properties}
    The following hold:
    \begin{itemize}
        \item[(i)] The objective function $f_\delta(\v t)$ in \eqref{eqn:opt-br2} is continuous on $[0,1]^p$.
        \item[(ii)] For every binary vector $\v s \in \{0, 1\}^p$ (i.e., a corner point on the hypercube $[0, 1]^p$),
        \[
        f_\delta(\v s) = - \v 1^\top  \Sigma_{[\v s]}^{-1} \v 1, \quad \text{for all}\,\, \delta > 0.
        \]
        \item[(iii)] For every fixed $\v t \in (0, 1)^p$, $f_\delta(\v t)$ is monotonically increasing in $\delta > 0$.
        \item[(iv)] For any $k =1, \dots, p$ and $\delta > 0$, 
        \[
        \minimize_{\v t \in \cC_k} f_\delta(\v t) = \minimize_{\v t \in \cS_k} f_\delta(\v t),
        \]
        here, the simplex $\cS_k = \{\v t \in [0, 1]^p : \v t^\top \v 1 =k \}$ corresponds to the polytope $\cC_k$ given in \eqref{eqn:defn-polytope}.
        \item[(v)]  Let $\eta_1$ be the largest eigenvalue of $\Sigma$. Then, 
        $f_\delta(\v t)$ strictly concave over $[0,1]^p$ for $\delta \geq \eta_1$.
    \end{itemize}
\end{theorem}

\subsection{Gradient}
We first derive a convenient closed--form expression for the gradient of $f_\delta$.


\begin{lemma}[Gradient of the relaxed objective]
\label{lem:grad-fdelta}
For each $\v t \in [0,1]^p$ and define
\(
\v x := \wt\Sigma_{\v t}^{-1}\v t
\)
and
\(
\v z := \Sigma (\v t \odot \v x).
\)
Then $f_\delta$ is differentiable at $\v t$ and
\begin{equation}
\label{eq:grad-vector}
\nabla f_\delta(\v t)
=
-2\,\v x
\;+\;
2\,\v x \odot \v z
\;-\;
2\delta\,\v t \odot \v x \odot \v x .
\end{equation}
Moreover, with
\(
\Pi_{\v t}
:= \Sigma + \delta(T_{\v t}^{-2} - I),
\)
on the interior points, the gradient admits the equivalent form
\begin{align}
\nabla f_\delta(\v t)
= -2\delta\,\frac{(\Pi_{\v t}^{-1}\v 1)^2}{\v t^3},
\qquad \v t\in(0,1)^p.
\label{eq:grad-vector-interior}
\end{align}
where all operations between vectors are elementwise.
\end{lemma}

\section{Algorithms for KKT-Minimal Points}
In this section, we develop several continuous optimization algorithms for the relaxed problem
\eqref{eqn:opt-br2} over the simplex $\cS_k$.  The algorithms operate by updating the regularization parameter $\delta$ during the
iterations and are designed to converge to KKT binary corner of $\cS_k$ at $\delta=\eta_1$,
yielding a candidate solution for the
original sparse portfolio problem.

\subsection{Frank-Wolfe Homotomy Method}
Algorithm~\ref{alg:alg} is a variant of the standard Frank-Wolfe algorithm, similar to the \texttt{Grid-FW} of \cite{moka2025scalable}. This algorithm is coupled with a continuation scheme in the regularization parameter
$\delta$. 

\begin{algorithm}
\caption{\texttt{FW-Homotopy}$(\Sigma, k, \alpha, n)$}
\label{alg:alg}
\begin{algorithmic}[1]
\State Compute the largest and smallest eigenvalues $\eta_1$ and $\eta_p$ of $\Sigma$
\State $\tau \leftarrow 10^{-4}$ \Comment{Tolerance for termination}
\State $\varepsilon \leftarrow 0.1 (k/p)$ 
\State $\delta_0 \leftarrow 3\eta_p\varepsilon^2/(1 + 3\varepsilon^2)$ 
\State $r \leftarrow (\eta_1/\delta_0)^{1/(n-1)}$
\State $\v t \leftarrow (k/p) \v 1$
\State $\ell \leftarrow 1$
\Repeat
    \State $\delta \leftarrow \delta_0 r^{\ell}$
    \State Compute the gradient $\nabla f_{\delta}(\v t)$
    \State Let $\v s \in \{0,1\}^p$ have ones at the positions of the $k$ smallest components of $\nabla f_{\delta}(\v t)$
    \State $\v t \leftarrow (1-\alpha)\v t + \alpha \v s$
    \State $\ell \leftarrow \ell + 1$
    \If{$\displaystyle \min_{j=1,\dots,p}\min\{t_j,\,1-t_j\}\le \tau$}
        \State Set $\delta \leftarrow \eta_1$ and compute $\v g \leftarrow \nabla f_{\delta}(\v s)$ 
        \If{$\displaystyle \max_{j:\,s_j=1} g_j \le \min_{i:\,s_i=0} g_i$} \Comment{KKT-Certification at $\delta=\eta_1$}
            \State \Return $\v s$
        \EndIf
    \EndIf
\Until{$\ell > n$}
\State \Return $\v s$
\end{algorithmic}
\end{algorithm}
\sbmc{[Explain the algorithm.]} 

\begin{appendix}
\section{Proofs}
\begin{proof}[Proof of Lemma~\ref{lem:grad-fdelta}]
For $\v t\in[0,1]^p$, since $\wt\Sigma_{\v t}$ invertible,
using the identity
\(
\partial A^{-1} = -A^{-1}(\partial A)A^{-1}
\)
for matrix differentials and the product rule, one obtains
\[
\frac{\partial f_\delta}{\partial t_i}
=
-2x_i
\;+\;
2x_i z_i
\;-\;
2\delta\, t_i x_i^2,
\]
where 
$\v x = \wt\Sigma_{\v t}^{-1}\v t$
and 
$\v z = \Sigma(\v t\odot\v x)$.
Collecting the components yields \eqref{eq:grad-vector}, which is well-defined for all such $\v t\in[0,1]^p$ (no division by $t_i$ is involved).

For the second expression, restrict to interior points $\v t\in(0,1)^p$, so that $T_{\v t}$ is invertible. Then
\[
\wt\Sigma_{\v t}
= T_{\v t}\Pi_{\v t}T_{\v t},
\qquad
\text{with}\,\,\,\Pi_{\v t}
:= \Sigma + \delta(T_{\v t}^{-2}-I),
\]
and hence
\[
\wt\Sigma_{\v t}^{-1}\v t
= T_{\v t}^{-1}\Pi_{\v t}^{-1}T_{\v t}^{-1}\v t
= T_{\v t}^{-1}\Pi_{\v t}^{-1}\v 1.
\]
Thus
\[
x_i = \frac{\bigl(\Pi_{\v t}^{-1}\v 1\bigr)_i}{t_i},
\qquad
t_i>0.
\]
Substituting this representation of $x_i$ (and the corresponding expression for $z_i$) into
\eqref{eq:grad-vector}, the terms $-2x_i+2x_i z_i$ cancel, yielding
\[
\frac{\partial f_\delta}{\partial t_i}
= -2\delta\, t_i x_i^2
= -2\delta\,\frac{\bigl(\Pi_{\v t}^{-1}\v 1\bigr)_i^2}{t_i^3},
\quad i=1,\dots,p.
\]
Writing this in vector form gives the stated alternative gradient expression for $\v t\in(0,1)^p$.
\end{proof}
\end{appendix}
\bibliographystyle{apalike}
\bibliography{Refs}
\end{document}

