\documentclass{article}
\usepackage[utf8]{inputenc}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}


% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{listings}
\usepackage{xcolor}

% Setup for Python code listings
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!60!black},
  showstringspaces=false,
  frame=single,
  columns=fullflexible
}

% Useful packages
\usepackage{color,tikz}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{caption,subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{varwidth}
%\def\Item$#1${\item $\displaystyle#1$
%   \hfill\refstepcounter{equation}(\theequation)}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{euscript}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[titletoc,title]{appendix}
\usepackage{upgreek}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{bbm}

\makeatletter
%\newcommand{\@giventhatstar}[2]{\left(#1\,\middle|\,#2\right)}
\newcommand{\@giventhatnostar}[3][]{#1(#2\,#1|\,#3#1)}
\newcommand{\giventhat}{\@ifstar\@giventhatstar\@giventhatnostar}
\makeatother

\renewcommand{\rmdefault}{ppl}

\usepackage{float}
\usepackage[scaled]{helvet}
\usepackage[utf8]{inputenc}
% \usepackage{algorithm}
% \usepackage[noend]{algorithmic}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \SetKwInput{KwInput}{Input}                % Set the Input
% \SetKwInput{KwOutput}{Output}     
% \SetKwInput{KwInit}{Initialization}     
\usepackage{algorithm,algpseudocode}% http://ctan.org/pkg/{algorithms,algorithmx}
%\algnewcommand{\Inputs}[1]{%
%  \State \textbf{Inputs:}
%  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
%}
%\algnewcommand{\Initialize}[1]{%
%  \State \textbf{Initialize:}
%  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
%}
\usepackage{natbib}
\usepackage{tcolorbox}
\newcommand{\brbox}{\begin{tcolorbox}[colback=blue!10!white,colframe=orange!50!black,title=Remark]}
\newcommand{\bibox}{\begin{tcolorbox}[colback=orange!10!white,colframe=blue!50!black,title=Idea]}
\newcommand{\bpbox}{\begin{tcolorbox}[colback=red!10!white,colframe=green!50!black,title=Problem]}
\newcommand{\ebox}{\end{tcolorbox}}

\definecolor{light-gray}{gray}{0.6} 
%\usepackage[color=red, fontsize=55pt, stamp=true, text=IN~PROGRESS]{draftwatermark}

%\textwidth 15.9cm \textheight 22.1cm \evensidemargin 0cm
%\oddsidemargin 0cm \topmargin 0cm
%\setlength{\parindent}{0pt}
%\renewcommand{\baselinestretch}{1.15}
%\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\vb}{\vspace{3mm}}



\newcommand{\DD}{{\rm d}}
\newcommand{\s}{^\star}
% \newcommand{\PP}{{\mathbb P}}
% \newcommand{\EE}{{\mathbb E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\xx}{\mathbb{X}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\pp}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ee}{\mathbb{E}}
\newcommand{\vt}{\vartheta}
\newcommand{\hN}{^{(N)}}
\newcommand{\rmd}{\mathrm{d}}


\newcommand{\scA}{\mathscr{A}}
\newcommand{\scB}{\mathscr{B}}
\newcommand{\scC}{\mathscr{C}}
\newcommand{\scD}{\mathscr{D}}
\newcommand{\scF}{\mathscr{F}}
\newcommand{\scG}{\mathscr{G}}
\newcommand{\scH}{\mathscr{H}}
\newcommand{\scI}{\mathscr{I}}
\newcommand{\scJ}{\mathscr{J}}
\newcommand{\scK}{\mathscr{K}}
\newcommand{\scL}{\mathscr{L}}
\newcommand{\scM}{\mathscr{M}}
\newcommand{\scP}{\mathscr{P}}
\newcommand{\scS}{\mathscr{S}}
\newcommand{\scR}{\mathscr{R}}
\newcommand{\scU}{\mathscr{U}}
\newcommand{\scE}{\mathscr{E}}
\newcommand{\scT}{\mathscr{T}}
\newcommand{\scV}{\mathscr{V}}
\newcommand{\scW}{\mathscr{W}}
\newcommand{\scX}{\mathscr{X}}
\newcommand{\scY}{\mathscr{Y}}
\newcommand{\scZ}{\mathscr{Z}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\bQ}{Q}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\intg}{\mathbb{Z}}
\newcommand{\natr}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\zlow}{z_{\sf low}}
\newcommand{\zup}{z_{\sf up}}
\newcommand{\elow}{E_{\sf low}}
\newcommand{\eup}{E_{\sf up}}

\newcommand{\mc}{\mathcal}
\newcommand{\m}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mbb}{\mathbb}
\newcommand{\mscr}{\mathscr}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\wb}{\overline}
\newcommand{\var}{\mathbb{V}ar}
\newcommand{\Vol}{\mathsf{Vol}}
\newcommand{\ebic}{\mathsf{EBIC}}

\renewcommand{\v}[1]{\boldsymbol{#1}}

\newcommand{\tr}{\mathsf{Tr}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\tht}{\boldsymbol{\theta}}
\newcommand{\hbeta}{\widehat \bbeta}
\newcommand{\tbeta}{\widetilde \bbeta}

\newcommand{\red}{\color{red}}
\newcommand{\hl}{\textcolor{blue}}
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\minimize}{min\,}
\DeclareMathOperator*{\maximize}{max\,}
\DeclareMathOperator*{\argmin}{argmin\,}
\DeclareMathOperator*{\subjectto}{subject\,\, to\,}
\DeclareMathOperator*{\diag}{Diag\,}

%%%%%%%%%%%%%
% Begin Sarat's definitions
%%%%%%%%%%%%%%%
\def\boxit#1{\vbox{\hrule\hbox{\vrule\kern6pt
          \vbox{\kern6pt#1\kern6pt}\kern6pt\vrule}\hrule}}
\def\sbmcomment#1{\vskip 2mm\boxit{\vskip -2mm{\color{cyan}\bf#1} {\color{cyan}\bf -- SBM\vskip 2mm}}\vskip 2mm}
\newcommand{\sbmc}[1]{{\color{cyan}#1}}
%%%%%%%%%%%%%
% End Sarat's definitions
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%
% Begin Matias's definitions
%%%%%%%%%%%%%%%
\def\boxit#1{\vbox{\hrule\hbox{\vrule\kern6pt
          \vbox{\kern6pt#1\kern6pt}\kern6pt\vrule}\hrule}}
\def\mqcomment#1{\vskip 2mm\boxit{\vskip -2mm{\color{red}\bf#1} {\color{red}\bf -- MQ\vskip 2mm}}\vskip 2mm}
\newcommand{\mqc}[1]{{\color{red}#1}}
%%%%%%%%%%%%%
% End Matias's definitions
%%%%%%%%%%%%%%%


\newtheorem{theorem}{Theorem}
\newtheorem{algorithm1}{Algorithm}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\title{Notes by Daniel}
% in the Artificial Intelligence Era
\author{}

\begin{document}

\maketitle


\section{Core Problem Formulation}
Consider a symmetric positive semi-definite matrix $A \in \reals^{p \times p}$ and a vector $\v b \in \reals^{p}$. For each binary vector $\v s \in \{0, 1\}^p$, let $A_{[\v s]}$ denote the principal submatrix of $A$ indexed by $\{j: s_j = 1\}$, and $\v b_{[\v s]}$ as the subvector of $\v b$ on the same index set. 
For a given positive integer $k \leq p$ (usually $k$ is much smaller than $p$), our aim is solve the sparse constrained problem:
\begin{align}
    \maximize_{\v s \in \{0, 1\}^p} \v b_{[\v s]}^\top (A_{[\v s]})^\dag \v b_{[\v s]}, \quad \subjectto\,\, \v |\v s| \leq k.
    \label{eqn:opt-master}
\end{align}
where $|\v s|$ denotes the number of ones in $\v s$ and $(A_{[\v s]})^\dag$ denotes the Moore-Penrose pseudo-inverse of $A_{[\v s]}$. It is important to keep in mind that $(A_{[\v s]})^\dag$ is not a submatrix of $A^\dag$. Note that if $A$ is full-rank (i.e., positive definite), each $A_{[\v s]}$ is invertible. Also note that each $A_{[\v s]}$ is (symmetric) positive semi-definite since it is a principal submatrix of $A$, which is assumed to be symmetric positive semi-definite. The problem \eqref{eqn:opt-master} is NP-hard and serves as a unifying framework for several important problems in statistics and machine learning. Below we detail how various domain-specific problems can be reformulated as instances of this core problem. We now list some of these problems.

\begin{example}({\bf Minimum-Variance Portfolio Selection:})
\normalfont
The classical minimum-variance portfolio optimization typically produces dense solutions with nonzero weights across all assets, which can lead to high transaction costs and increased estimation error. To address these limitations, sparse portfolio selection incorporates an explicit cardinality constraint that limits the number of assets to at most \(k\). This yields the optimization problem gievn by
\begin{align}
    \minimize_{\v \beta \in \reals^p} \v \beta^\top \Sigma \v \beta, \quad \subjectto \v 1^\top \v \beta = 1, \,\, \|\v \beta\|_0 \leq k,
    \label{eqn:sparse-portfolio}
\end{align}
where \(\Sigma \in \reals^{p \times p}\) is an invertible covariance matrix of asset returns.
This formulation can be equivalently expressed as a binary-constrained optimization problem:
\begin{align}
    \minimize_{\substack{\v s \in \{0, 1\}^p \\ |\v s| \leq k}} \minimize_{\substack{\v \beta_{[\v s]} \in \reals^{|\v s|} \\ \v 1^\top \v \beta_{[\v s]} = 1}} \v \beta_{[\v s]}^\top \Sigma_{[\v s]} \v \beta_{[\v s]}. 
    \label{eqn:sparse-portfolio-binary}
\end{align}
For any fixed support \(\v s\), the inner minimization admits a closed-form solution,  given by
\[
\v \beta_{[\v s]}^* = \frac{(\Sigma_{[\v s]})^{-1} \v 1}{\v 1^\top (\Sigma_{[\v s]})^{-1} \v 1},
\]
yielding the minimal portfolio variance 
\(
\nicefrac{1}{\v 1^\top (\Sigma_{[\v s]})^{-1} \v 1}.
\)
Thus, the combinatorial optimization problem reduces to
\begin{align}
    \maximize_{\v s \in \{0, 1\}^p} \v 1^\top (\Sigma_{[\v s]})^{-1} \v 1, \quad \subjectto |\v s| \leq k.
    \label{eqn:sparse-portfolio-master}
\end{align}

This precisely matches the form of the core problem \eqref{eqn:opt-master} with \(A = \Sigma\) and \(\v b = \v 1\), demonstrating that sparse portfolio selection is another important instance of the core problem.
\end{example}

\noindent
{\bf A Crucial Observation:}
Recall the core problem \eqref{eqn:opt-master}. Assume that $A$ has a full rank (i.e., positive definite) and all the elements of $\v b$ are non-zero. Then, for every $\v s\in \{0, 1\}^p$,
\begin{align*}
    \v b_{[\v s]}^\top (A_{[\v s]})^\dag \v b_{[\v s]} 
    &=  \v b_{[\v s]}^\top (A_{[\v s]})^{-1} \v b_{[\v s]}\\
    &=  \v b_{[\v s]}^\top (A_{[\v s]})^{-1} \v b_{[\v s]}\\
    &=  \v 1^\top \diag(\v b_{[\v s]}) (A_{[\v s]})^{-1} \diag(\v b_{[\v s]}) \v 1\\
    &=  \v 1^\top \lt(\diag(\v 1/\v b_{[\v s]}) A_{[\v s]} \diag(\v 1/\v b_{[\v s]}) \rt)^{-1} \v 1\\
    &=  \v 1^\top \lt(\Sigma_{[\v s]}\rt)^{-1} \v 1,
\end{align*}
where $\Sigma = \diag(\v 1/\v b) A \diag(\v 1/\v b)$. This implies that, under the minor assumption, solving the minimum-variance portfolio problem is equivalent to solving the core problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
\normalfont
{\bf Column Subset Selection Problem:}
Let \(X \in \reals^{m \times p}\) be a data matrix and define \(X_{[:,\v s]}\) as the submatrix with the selected columns associated with ones in $\v s$. Further define the (orthogonal) projector
\[
P_{\v s} := X_{[:,\v s]}\bigl(X_{[:,\v s]}^\top X_{[:,\v s]}\bigr)^{\dagger} X_{[:,\v s]}^\top.
\]
The \emph{column subset selection problem (CSSP)} in Frobenius norm is
\begin{align}
\min_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;\bigl\|\,X - P_{\v s} X\,\bigr\|_F^2.
\label{eq:cssp-frob}
\end{align}
Using \(\|B\|_F^2 = \tr(B^\top B)\) and the idempotence of \(P_{\v s}\), we obtain 
\begin{align*}
%\min_{\v s \in \{0, 1\}^p:\,|S|\le k} \;
\bigl\|X - P_{\v s} X\bigr\|_F^2
= \|X\|_F^2 - 
%\max_{\v s \in \{0, 1\}^p:\,|S|\le k} 
\tr\!\bigl(X^\top P_{\v s} X\bigr),
\end{align*}
which implies, \eqref{eq:cssp-frob} is equivalent to
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \tr\!\bigl(X^\top P_{\v s} X\bigr).
\label{eq:cssp-trace}
\end{align}
Introduce a Rademacher vector \(\v \xi \in \{\pm1\}^n\) with \(\ee[\v \xi \v \xi^\top]=I\) (i.e, generate elements of \(\v \xi\) independently and uniformly on $\{-1, +1\}$ ).
By Hutchinson's identity \cite{hutchinson1989stochastic}, 
\(\tr(B)=\ee_{\v \xi}[\v \xi^\top B \v \xi]\) for any square \(B\),
so \eqref{eq:cssp-trace} can be written as
\begin{align}
\tr\!\bigl(X^\top P_{\v s} X\bigr) = \ee_{\v \xi}\!\Big[\,
\v \xi^\top X^\top P_{\v s} X \v \xi
\,\Big].
\label{eq:cssp-hutch}
\end{align}
Now set
\[
A := X^\top X \in \reals^{n\times n},
\qquad
\v b^{(\v \xi)} := X^\top X\,\v \xi \in \reals^{n},
\]
so that \(A_{[\v s]} = X_{[:,\v s]}^\top X_{[:,\v s]}\) and
\(
\v b^{(\v \xi)}_{[\v s]} = X_{[:,\v s]}^\top X \xi.
\)
Then the inner quadratic in \eqref{eq:cssp-hutch} becomes
\[
\v \xi^\top X^\top P_{\v s} X \v \xi
= \bigl(\v b^{(\v \xi)}_{[\v s]}\bigr)^\top \bigl(A_{[\v s]}\bigr)^{\dagger}\, \v b^{(\v \xi)}_{[\v s]},
\]
and the CSSP objective can be expressed as the following expected instance of our core problem:
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
\ee_{\v \xi}\!\Big[\,
\bigl(\v b^{(\v \xi)}_{[\v s]}\bigr)^\top\bigl(A_{[\v s]}\bigr)^{\dagger}\, \v b^{(\v \xi)}_{[\v s]}
\,\Big].
\label{eq:cssp-core-expected}
\end{align}
\end{example}
{\color{blue}
\paragraph{Example (Hutchinsonâ€™s Trace Identity).}
Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. Let $z \in \mathbb{R}^n$ be a random vector whose entries are independent Rademacher random variables, i.e.,
\[
\mathbb{P}(z_i = 1) = \mathbb{P}(z_i = -1) = \tfrac{1}{2}, \quad i = 1, \dots, n.
\]
Then the following identity holds:
\[
\mathbb{E}\!\left[z^\top A z\right] = \operatorname{tr}(A).
\]

\emph{Proof.}
By direct expansion,
\[
z^\top A z = \sum_{i=1}^n \sum_{j=1}^n A_{ij} z_i z_j.
\]
Taking expectation and using independence,
\[
\mathbb{E}[z_i z_j] =
\begin{cases}
1, & i = j, \\
0, & i \neq j,
\end{cases}
\]
we obtain
\[
\mathbb{E}\!\left[z^\top A z\right]
= \sum_{i=1}^n A_{ii}
= \operatorname{tr}(A).
\]
\hfill$\square$
}

{\red Daniel will work on column subset selection problem focusing on:
\begin{itemize}
    \item Rewrite the following results,
    \item Rewrite the algorithm,
    \item Investigate improvements techniques,
    \item Running simulations to compare with existing methods.
\end{itemize}}

\section{CSSP - Daniel}
\subsection{Boolean Relaxation v2.}
\subsubsection{Reformulation}
{\bf Assumption 1:}
\textit{Matrix $A = X^{\top}X$ is positive definite}.

This assumption is not restricitive, because we can perform a 
ridge regularization, by replacing $A$ with $A + \lambda I$ for some $\lambda > 0$ (Moka et al., 2025). ( Also cited (Fastrich, Paterlini and Winker, 2015))

Now, by expanding expectation, \eqref{eq:cssp-core-expected} can be reforumlated into
\begin{align}
\label{eq:cssp-hadamard-reform}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
\ee_{\v \xi}\!\Big[\,
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
(A_{[\v s]})^{-1}
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})
\,\Big]
% =
% (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
% (A_{[\v s]})^{-1}
% (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})
% &\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k}
% \sum_{i = 1}^{2^n} \frac{1}{2^n} 
% (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
% (A_{[\v s]})^{-1}
% (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]}).
% &\frac{1}{2^n}
% \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k}
% \sum_{i = 1}^{2^n} 
% \v\xi_i^{\top} X^{\top} X_{[:, \v s]} (A_{[\v s]})^{-1} 
%  X_{[:, \v s]}^{\top} X \v\xi_i,
\end{align} 

We propose the boolean relaxation of \eqref{eq:cssp-hadamard-reform} as,
\begin{align}
\label{eq:cssp-boolean-relax}
\maximize_{\v t \in \cC} \;
g_\delta(\v t) 
=
\max_{\v t \in \cC} \;
\ee_{\v \xi}\!\Big[\,
f_\delta (\v t, \v \xi)
\,\Big]
\end{align},
where $f_\delta (\v t, \v \xi) = [b^{(\v \xi)} \odot \v t]^{\top} (\wt A_{\v t})^{-1} [b^{(\v \xi)} \odot \v t]$,
and $\wt A_{\v t} = T_{\v t} A T_{\v t} + \delta (I - T_{\v t}^2 ).$


% Hence, \eqref{eq:cssp-core-expected} can be further simplified to 
% \begin{align}
% \label{eq:cssp-core-reform}
% \frac{1}{2^n}
% \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k}
% \sum_{i = 1}^{2^n} 
% \v\xi_i^{\top} \Sigma_{[\v s]}^{-1} \v\xi_i 
% =
% \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k}
% \frac{1}{2^n}
% \sum_{i = 1}^{2^n} 
% (\v 1 \odot \v\xi_i)^{\top} \Sigma_{[\v s]}^{-1} 
% (\v 1 \odot \v\xi_i)
% \end{align}
% where
% $\Sigma_{[\v s]}^{-1} = X^{\top} X_{[:, \v s]} (A_{[\v s]})^{-1} 
%  X_{[:, \v s]}^{\top} X.$

% Note that, $\Sigma_{[\v s]}$ is constant in terms of $\v \xi$, and it's only dependent on a choice vector $\v s$.

% \subsubsection{Boolean Relaxation}
% From the scalable-gradient paper's scaling part, boolean relaxation of single term of above is already achieved, where
% $\v w = \v \xi_i$ and $R_{[\v s]}^{-1} = \Sigma_{[\v s]}^{-1}$.
% Using this, we suggest the relaxation of \eqref{eq:cssp-core-reform} as 
% \begin{align}
% \max_{\v t \in \cC_k} g_\delta (\v t)
% = 
% \max_{\v t \in \cC_k}
% \ee_{\v \xi}\!\Big[\, f_\delta (\v t, \v \xi) \, \Big]
% =
% \max_{\v t \in \cC_k}
% \frac{1}{2^n}
% \sum_{i = 1}^{2^n}  f_\delta(\v t, \v \xi_i)
% \end{align}
% where
% \[
% f_\delta (\v t, \v \xi_i)
% = (\v \xi_i \odot \v t)^{\top}
% \wt \Sigma_{\v t}^{-1}
% (\v \xi_i \odot \v t)
% \]
% and
% \[
% \wt \Sigma_{\v t} = T_{\v t} \Sigma T_{\v t} + \delta (I - T_{\v t}^2).
% \], which is constant in terms of $\v \xi$.

\subsubsection{Proof.}
From the scalable-gradient paper, we already know that there exists a valid boolean relaxation for a original problem of form $\minimize_{\v \in \{ 0, 1\}^p} - \v1^{\top} \Sigma^{-1}_{[\v s]} \v 1$.

Now, let $\Sigma^{-1}_{[\v s]} = \diag(\v b_{[\v s]}^{(\v \xi_i)}) A_{[\v s]}^{-1} \diag(\v b_{[\v s]}^{(\v \xi_i)})$, then orignal problem is equivalent to, 
\begin{align*}
    \minimize_{\v s \in \{ 0, 1\}^p} - \v1^{\top} \Sigma^{-1}_{[\v s]} \v 1 
    &=
    \maximize_{\v s \in \{ 0, 1\}^p}  \v1^{\top} \Sigma^{-1}_{[\v s]} \v 1 \\
    &=
    \maximize_{\v s \in \{ 0, 1\}^p}  \v1^{\top} \diag(\v b_{[\v s]}^{(\v \xi_i)}) A_{[\v s]}^{-1} \diag(\v b_{[\v s]}^{(\v \xi_i)}) \v 1 \\
    &=
    \maximize_{\v s \in \{ 0, 1\}^p}  (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
    A_{[\v s]}^{-1} (\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]}),
\end{align*}
using $\diag(\v b_{[\v s]}^{(\v \xi_i)}) \v 1 = \v 1 \odot \v b^{(\v \xi_i)}_{[\v s]}$, and the fact that $\Sigma$ is a positive definite matrix since $A$ is a positive definite matrix.

Hence, every problem is a special form of minimum-variance portfolio selection problem,
\begin{align}
\max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
(A_{[\v s]})^{-1}
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})
\end{align}
and can be relaxed into 
\begin{align}
\maximize_{\v t \in \cC_k} f_\delta (\v t)
\end{align}
and this relaxation satisfies all the properties of \eqref{thm:relaxation-properties}. 
This is because matrix $A$ is a positive definite, symmetric matrix, so $\Sigma$ from the original article can be successfully replaced with our $A$.




\subsubsection{(i)}
Continuity of $g_\delta (\v t)$ is from the fact that each $f_\delta (\v t , \v \xi)$ is continuous on $[0, 1]^p$, hence summation of $2^n$ continuous functions will also be continuous in the same hypercube.

\subsubsection{(ii)}


For any binary vector $\v s \in \{ 0, 1\}^p$, 
\begin{align*}
    (\wt A_{\v s})_{[ \v s]} = (T_{\v s} A T_{\v s})_{[ \v s]} = A_{[\v s]} \quad \mathrm{and} \quad (\wt A_{\v s})_{[ \v 1 - \v s]} = \delta I.
\end{align*}
Hence, $f_\delta( \v s) = [b^{(\v \xi)} \odot \v s]^T A_{[\v s]}^{-1} [b^{(\v \xi)} \odot \v s].$

As a consequence, $g_\delta (\v t)$ is equal to
\begin{align*}
    g_\delta (\v s)
    &=
    \ee_{\v \xi} \Bigg[f_{\delta}(\v t, \v\xi)\Bigg] \\
    &=
    \frac{1}{2^n}
    \sum_{i=1}^{2^n} f_\delta (\v s, \v \xi_i) 
    =
    \sum_{i=1}^{2^n} [b^{(\v \xi_i)} \odot \v s]^T A_{\v s}^{-1} [b^{(\v \xi_i)} \odot \v s]
    = \ee_{\v \xi} \! \Big[\ [b^{(\v \xi_i)} \odot \v s]^T A_{\v s}^{-1} [b^{(\v \xi_i)} \odot \v s] \Big],
\end{align*}
, since $\mathbb{P}( \v \xi = \v \xi_i) = \frac{1}{2^n}$.
Hence, we showed that at every corner point on the hypercube, our relaxation's cost function is equivalent to original problem.

\subsubsection{(iii)}
Lemma 2 from the scalable-gradient paper can be applied to our case.

Since $\v t = \v 1^{\top} T_{\v t}$, $\v b^{(\v \xi)} = (\v b^{\v \xi} \odot \v 1^{\top} T_{\v t}) = (\v b^{\v \xi} \odot \v 1 ) T_{\v t}$ holds.
Hence,
\begin{align*}
    f_\delta (\v t, \v \xi) 
    &= 
    (\v b^{\v \xi} \odot \v 1 )^{\top} T_{\v t}
    \wt A_{\v t}^{-1}
    T_{\v t} (\v b^{\v \xi} \odot \v 1 )
    \\
    &=
    (\v b^{\v \xi} \odot \v 1 )^{\top} (T_{\v t}^{-1}
    \wt A_{\v t}
    T_{\v t}^{-1})^{-1} 
    (\v b^{\v \xi} \odot \v 1 )
    \\
    &=
    (\v b^{\v \xi} \odot \v 1 )^{\top} 
    (A + \delta (T_{\v t}^{-2} - I))^{-1}
    (\v b^{\v \xi} \odot \v 1 )
\end{align*}
That is, for every interior point $\v t \in (0,1)^p$, our objective function $f_\delta (\v t)$ can be expressed as
\begin{align*}
    f_\delta (\v t, \v \xi) = [b^{(\v \xi)} \odot \v 1]^T \Pi_{\v t}^{-1} [b^{(\v \xi)} \odot \v 1],
\end{align*}
where 
\begin{align*}
\Pi_{\v t} = A + \delta D_{\v t}
\quad \mathrm{and} \quad
D_{\v t} = T_{\v t}^{-2} - I.
\end{align*}

Using above result, we get
\begin{align*}
    \frac{d}{d\delta}f_\delta (\v t, \v \xi)
    &= [b^{(\v \xi)} \odot \v 1]^{\top} \;
    (\frac{d}{d\delta} \Pi_{\v t}^{-1}) \;
    [b^{(\v \xi)} \odot \v 1] \\
    &= -[b^{(\v \xi)} \odot \v 1]^{\top} \;
    \Pi_{\v t}^{-1} (\frac{d}{d\delta} \Pi_{\v t})\Pi_{\v t}^{-1} \;
    [b^{(\v \xi)} \odot \v 1] \\
    &= -\v u^{\top} \;
    (\frac{d}{d\delta} \Pi_{\v t}) \;
    \v u \\
    &=\v u^{\top} (I - T_{\v t}^{-2}) \v u,
\end{align*}
with $\v u = \Pi_{\v t}^{-1}[b^{(\v \xi)} \odot \v 1] \neq \v 0$.
Since $\v t$ is in unit interval, the matrix $I - T_{\v t}^{-2}$ in the quadratic form is negative definite. 
Hence, $f_\delta (\v t)$ is monotonically \textit{decreasing} in $\delta > 0$, successfully penalising our value.


% Since $\v t$ is in unit inverval, matrix $(T_{\v t}^{-2} - I)$ is always positive definite. Hence, 



% Similarly, each $f_\delta(\v t, \v \xi_i)$ is monotonically decreasing in $\delta > 0$, so its summation will be monotonically decreasing as well.

\subsubsection{(iv)}
We can find the gradient our relaxed objective using \eqref{lem:grad-fdelta}, with minor adjustment.

For every interior point $\v t \in (0,1)^p$, our objective function $f_\delta (\v t)$ can be expressed as
\begin{align*}
    f_\delta (\v t, \v \xi) = [b^{(\v \xi)} \odot \v 1]^T \Pi_{\v t}^{-1} [b^{(\v \xi)} \odot \v 1].
\end{align*}
Thus, for each $j = 1, \dots , p$, 
\begin{align*}
    \frac{\partial f_{\delta}(\v t, \v \xi)}{\partial t_j} 
    &= [b^{(\v \xi)} \odot \v 1]^T \;\frac{\partial \Pi_{\v t}^{-1}}{\partial t_j} \;
    [b^{(\v \xi)} \odot \v 1] \\
    &= - [b^{(\v \xi)} \odot \v 1]^T \;
    \Pi_{\v t}^{-1} \frac{\partial \Pi_{\v t}}{\partial t_j} \Pi_{\v t}^{-1} \;
    [b^{(\v \xi)} \odot \v 1].
\end{align*}
Additionally, since $A$ doesn't depend on $\v t$ from our definition $\Pi_{\v t} = A + \delta D_{\v t}$, 
\begin{align*}
    \frac{\partial \Pi_{\v t}}{\partial t_j}
    = \delta \frac{\partial D_{\v t}}{\partial t_j} = -\frac{2 \delta}{t_j^3} \v e_j \v e_j^{\top}.
\end{align*}
Thus, 
\begin{align*}
\nabla f_\delta(\v t, \v \xi)
= 2\delta\,\frac{(\Pi_{\v t}^{-1}\v [b^{(\v \xi)} \odot \v 1])^2}{\v t^3},
\qquad \v t\in(0,1)^p.
\end{align*}
% where $\v v = \frac{1}{\v b^{\v \xi} \odot \v b^{\v \xi}}$

For interior points,
\begin{align*}
    \nabla g_\delta (\v t) 
    = \nabla (\frac{1}{2^n} \sum_{i = 1}^{2^n} f_\delta (\v t, \v \xi_i))
    &= \frac{1}{2^n} \sum_{i = 1}^{2^n} \nabla f_\delta (\v t, \v \xi_i) \\
    &= \frac{1}{2^n} \sum_{i = 1}^{2^n} 2\delta\,\frac{(\Pi_{\v t}^{-1}\v [b^{(\v \xi_i)} \odot \v 1])^2}{\v t^3}.
\end{align*}

Note that, all the elements of gradient $\nabla g_\delta (\v t, \v \xi)$ are non-negative since each $\nabla f_\delta (\v t, \v \xi)$ are non-negative.
Hence, objective function $g_\delta (\v t, \v \xi)$ is non-decreasing along $t_j$.
Hence, simplex $\cS_k$ contains an optimal solution of boolean relaxation.


% Since in lemma 3, we showed that every element is non-negative, its summation will also be non-negative. (Note that, in the paper, function had negative sign in front of it.) And hence, $g_\delta$ will also be non-decreasing, so optimal solution is contained by the simplex $\cS_k$.


\subsubsection{(v)}
Hessian of $f_\delta (\v t, \v \xi$ for every interior point $\v t \in (0, 1)^p$ is gien by 
\begin{align*}
    H_\delta (\v t, \v \xi) = 2 \delta \diag(\frac{\v u_{\v t, \v \xi}}{\v t ^3}) 
    \; [4 \delta \Pi_{\v t}^{-1} - 3 \diag(\v t^2)] \;
    \diag(\frac{\v u_{\v t, \v \xi}}{\v t ^3}),
\end{align*}
where $\Pi_{\v t} = A + \delta D_{\v t}
\quad \mathrm{and} \quad
D_{\v t} = T_{\v t}^{-2} - I$ and $\v u_{\v t, \v \xi} = \Pi_{\v t}^{-1} [\v b^{(\v \xi)} \odot \v 1] =  \Pi_{\v t}^{-1}\v b^{(\v \xi)}$.

\textit{proof.}

Using \begin{align*}
\nabla f_\delta(\v t, \v \xi)
= 2\delta\,\frac{(\Pi_{\v t}^{-1}\v [b^{(\v \xi)} \odot \v 1])^2}{\v t^3},
\qquad \v t\in(0,1)^p.
\end{align*},
we can express derivative to $t_j$ as below,
\begin{align*}
    \frac{\partial}{\partial t_j} \nabla f_\delta (\v t, \v \xi)
    &= \frac{-6 \delta}{t_j^4} \v e_j \v e_j^{\top} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) 
    + 2\delta (\frac{1}{\v t^3}) \odot \frac{\partial}{\partial t_j} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) \\
    &= \frac{-6 \delta}{t_j^4} \v e_j \v e_j^{\top} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) 
    + 4\delta (\frac{u_{\v t, \v \xi}}{\v t^3}) \odot \frac{\partial u_{\v t, \v \xi}}{\partial t_j} \\
    &= \frac{-6 \delta}{t_j^4} \v e_j \v e_j^{\top} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) 
    - 4\delta (\frac{u_{\v t, \v \xi}}{\v t^3}) \odot [\Pi_{\v t}^{-1} \frac{\partial \Pi_{\v t}^{-1}}{\partial t_j} \Pi_{\v t}^{-1} \v b^{(\v \xi)}]\\
    &= \frac{-6 \delta}{t_j^4} \v e_j \v e_j^{\top} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) 
    + 4\delta (\frac{u_{\v t, \v \xi}}{\v t^3}) \odot [\Pi_{\v t}^{-1} \frac{2 \delta}{t_j^3} \v e_j \v e_j^{\top}
    \Pi_{\v t}^{-1} \v b^{(\v \xi)}]
 \end{align*}
 using $\frac{\partial \Pi_{\v t}}{\partial t_j}
    = \delta \frac{\partial D_{\v t}}{\partial t_j} = -\frac{2 \delta}{t_j^3} \v e_j \v e_j^{\top}.$

Further expanding, above reformulates to 
\begin{align*}
\frac{\partial}{\partial t_j} \nabla f_\delta (\v t, \v \xi) = 
\frac{-6 \delta}{t_j^4} \v e_j \v e_j^{\top} (\v u_{\v t, \v \xi} \odot \v u_{\v t, \v \xi}) 
    + \frac{8 \delta^2}{t_j^3} (\frac{u_{\v t, \v \xi}}{\v t^3}) \odot [\Pi_{\v t}^{-1} \v e_j \v e_j^{\top} \v u_{\v t, \v \xi}].
\end{align*}

Hence, Hessian of $f_\delta (\v t, \v \xi)$ is given by 
\begin{align*}
    H_{f, \delta}(\v t, \v \xi) 
    &= -6 \delta [ \diag( \frac{\v u_{\v t, \v \xi}}{\v t^3})]^2 T_{\v t}^2 + 8 \delta^2 \diag(\frac{\v u_{\v t, \v \xi}}{\v t^3}) \Pi_{\v t}^{-1} \diag(\frac{\v u_{\v t, \v \xi}}{\v t^3}) \\
    &= 2 \delta \diag( \frac{\v u_{\v t, \v \xi}}{\v t^3})
    [4 \delta \Pi_{\v t}^{-1} - 3 \diag{(\v t^2})]
    \diag( \frac{\v u_{\v t, \v \xi}}{\v t^3}).
\end{align*}

Hence, we can also get Hessian of $g_\delta(\v t)$ as,
\begin{align*}
    H_{g, \delta} (\v t) 
    &= \frac{1}{2^n}\sum_{i = 1}^{2^n} 
    H_{f, \delta}(\v t, \v \xi)
\end{align*}

Let $\eta_1, \eta_p$ be the largest and smallest eigenvalues of $A$, respectively.
Then, for each $f_\delta (\v t, \v \xi)$, below holds.

1. $f_\delta (\v t, \v \xi)$ is strictly convex over $[0,1]^p$ for $\delta \geq \eta_1$

2. For any $\epsilon \in (0,1)$, $f_\delta (\v t, \v \xi)$ is strictly concave over $[\epsilon, 1]^p$ for $\delta \leq 3 \eta_p \epsilon^2 / (1 + 3\epsilon^2)$.

Above is from theorem 3 of Scalable-Gradient.
Since our $A$ is a constant in terms of $\v \xi$ and $\v t$, this is true for each $2^n$ terms of $f_\delta (\v t, \v \xi_i)$.

Since finite sum of strictly convex / concave functions will also be a strictly convex / concave functions, we know that,

1. $g_\delta (\v t)$ is strictly convex over $[0,1]^p$ for $\delta \geq \eta_1$

2. For any $\epsilon \in (0,1)$, $g_\delta (\v t)$ is strictly concave over $[\epsilon, 1]^p$ for $\delta \leq 3 \eta_p \epsilon^2 / (1 + 3\epsilon^2)$.


% \subsection{Boolean Relaxation v1.}We make the following key assumption.
% \noindent 

% {\bf Assumption 1:}
% \textit{Matrix $A = X^{\top}X$ is positive definite}.

% This assumption is not restricitive, because we can perform a 
% ridge regularization, by replacing $A$ with $A + \lambda I$ for some $\lambda > 0$ (Moka et al., 2025). ( Also cited (Fastrich, Paterlini and Winker, 2015))

% Note that under this assumption, all elements of $\v b^{(\v \xi)} = (A + \lambda I ) \v \xi$ are non-zero.
% This is because,
% \begin{align*}
% &[\v b^{(\v \xi)}]_i = 0 \\
% &\iff [(A + \lambda I ) \v \xi]_i = 0 \\
% &\iff (A\v\xi)_i + \lambda \xi_i = 0 \\
% &\iff \lambda = -\frac{(A \v\xi)_i}{\xi_i}.
% \end{align*}

% Hence, we can choose $\lambda  > 0$ to be a value such that $\lambda \neq -\frac{(A \v\xi)_i}{\xi_i}$ for $i = 1, ... n$, while $A + \lambda I$ is positive definite. 

% Now, we showed that $\diag(\v 1 / \v b^{(\v \xi)}_{[\v s]})$ is well-defined and thus, can reformulate \eqref{eq:cssp-core-expected} into 
% \begin{align}
% \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \ee_{\v \xi} \! \Big[\ \v 1^\top (\Sigma^{(\v \xi)}_{[\v s]})^{-1} \v 1 \Big],
% \label{eq:cssp-sp-reform}
% \end{align}
% where \[
% \Sigma^{(\v \xi)}_{[\v s]} := \diag(\v 1 / \v b^{(\v \xi)}_{[\v s]}) \bigl(A_{[\v s]}\bigr) \diag(\v 1 / \v b^{(\v \xi)}_{[\v s]}).
% \]


% Note that, $\bigl( A_{[\v s]} \bigr)^{\dagger} = \bigl( A_{[\v s]} \bigr)^{-1}$ because $A$ is a positive-definite matrix, so each $A_{[\v s]}$ is invertible.
% Additionally, $\diag(\v 1 / \v b^{(\v \xi)}_{[\v s]})$ is well-defined because all elements of $\v b^{\v \xi}$ are non-zero.

% Using the fact that $\mathbb{P} (\v\xi = \v\xi_i) = \frac{1}{2^n}$, above \eqref{eq:cssp-sp-reform} can be reformulated into,
% \begin{align}
% \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} 
% \sum_{i=1}^{2^n} \frac{1}{2^n}
% (\v 1^\top (\Sigma^{(\v \xi_i)}_{[\v s]})^{-1} \v 1) 
% = \frac{1}{2^n} \max_{\v s \in \{0, 1\}^p:\,|\v s|\le k} 
% \sum_{i=1}^{2^n} 
% (\v 1^\top (\Sigma^{(\v \xi_i)}_{[\v s]})^{-1} \v 1) 
% ,
% \end{align} where $\v \xi_i$ is the $i$th Rademacher vector.

% We propose the relaxation as following.
% \begin{align}
% \label{eqn:cssp-bool-relax}
%     \maximize_{\v t \in \cC_k} g_\delta (\v t, \v \xi) = 
%     \maximize_{\v t \in \cC_k} \ee_{\v \xi} \Bigg[f_{\delta}(\v t, \v\xi)\Bigg], \quad \text{where}\,\,\,\,
% f_\delta(\v t, \v \xi) = \v t^\top ({\wt \Sigma^{(\v \xi)}}_{[\v t]})^{-1} \v t,
% \end{align}
% and 
% \begin{align}
% {\wt \Sigma^{(\v \xi_i)}}_{[\v t]} = T_{\v t} \Sigma^{(\v\xi_i)} T_{\v t} + \delta (I - T_{\v t}^2).
% \end{align}

% That is, boolean relaxation of minimum variance problem's expected value over a random Rademacher vector is actually a minimum variance problem's expected value of its relaxation.

% Now we expect to show that above is indeed a boolean relaxation by proving it satisfies all of \eqref{thm:relaxation-properties}.

% \subsection{Proof of boolean relaxation}
% Note that, we changed (strictly) concave to (strictly) convex and non-positive gradient to non-negative gradient because in the original paper, cost function $f_\delta(\v t)$ had negative sign in front of it. That is, in the original paper, our goal was $\minimize_{\v t \in \cC_k} (-f_\delta (\v t))$.

% \subsubsection{(i)}
% Continuity of $g_\delta (\v t, \v \xi)$ is from the fact that each $f_\delta (\v t , \v \xi)$ is continuous on $[0, 1]^p$, hence summation of $2^n$ continuous functions will also be continuous in the same hypercube.

% \subsubsection{(ii)}
% Using (ii) from Theorem 1, 
% Note that, $g_\delta (\v t, \v \xi)$ from \eqref{eqn:cssp-bool-relax} is equal to
% \begin{align}
%     g_\delta (\v t, \v \xi)
%     &=
%     \ee_{\v \xi} \Bigg[f_{\delta}(\v t, \v\xi)\Bigg] \\
%     &=
%     \frac{1}{2^n}
%     \sum_{i=1}^{2^n} f_\delta (\v t, \v \xi_i) 
%     =
%     \sum_{i=1}^{2^n} \v 1^T (\Sigma^{(\v \xi_i)}_{[\v s]})^{-1} \v 1
%     = \ee_{\v \xi} \! \Big[\ \v 1^\top (\Sigma^{(\v \xi)}_{[\v s]})^{-1} \v 1 \Big],
% \end{align}
% by applying (ii) from Theorem 1 to each $2^n$ terms, and since $\mathbb{P}( \v \xi = \v \xi_i) = \frac{1}{2^n}$.
% Hence, we showed that at every corner point on the hypercube, our relaxation's cost function is equivalent to original problem.

% \subsubsection{(iii)}

% From lemma 2 of scalable gradient, we know that for every interior point $\v t \in (0,1)^p$,
% \begin{align}
%     f_\delta ( \v t , \v \xi) 
%     = \v 1^{\top} \Pi_{\v t, \v \xi}^{-1} \v 1
% \end{align}
% where
% \begin{align}
%     \Pi_{\v t, \v \xi} 
%     :=
%     \Sigma^{(\v \xi)} + \delta (T_{\v t}^{-2} - I )
%     \quad
%     \text{ and }
%     \quad
%     D_{\v t} = T_{\v t}^{-2} - I.
% \end{align}

% Also, from the proof of (iii) from scalable-gradient, we also know that
% \begin{align}
%     \frac{d}{d\delta} f_\delta (\v t, \v \xi) 
%     = \v u_{\v t, \v \xi}^\top (I - T_{\v t}^{-2}) \v u_{\v t, \v \xi}
% \end{align}
% with $\v u_{\v t, \v \xi} = \Pi_{\v t, \v \xi}^{-1} \v 1 \neq \v 0$.

% Hence, using above, we can find $\frac{dg}{d\delta}$, which is
% \begin{align}
%     \frac{d}{d\delta} g_\delta (\v t, \v \xi) 
%     &= \frac{d}{d\delta} (\frac{1}{2^n} \sum_{i = 1}^{2^n} f_\delta (\v t, \v \xi_i)) \\
%     &= \frac{1}{2^n}\sum_{i = 1}^{2^n} \frac{d}{d\delta} f_\delta (\v t, \v \xi_i) \\
%     &= \frac{1}{2^n} \sum_{i = 1}^{2^n} \v u_{\v t, \v \xi_i}^\top (I - T_{\v t}^{-2}) \v u_{\v t, \v \xi_i}.
% \end{align}
% Note that, for every $i \in \{1, \dots 2^n\}$, $\frac{d}{d\delta}f_\delta (\v t, \v \xi_i) < 0$, so $\frac{d}{d \delta} g_\delta (\v t, \v \xi)$ is also negative.
% That is, for every fixed $\v t \in (0, 1,)^p$,
% $g_\delta (\v t, \v \xi)$ is monotonically decreasing in $\delta > 0$

% % Since $\v t$ is in unit inverval, matrix $(T_{\v t}^{-2} - I)$ is always positive definite. Hence, 



% % Similarly, each $f_\delta(\v t, \v \xi_i)$ is monotonically decreasing in $\delta > 0$, so its summation will be monotonically decreasing as well.

% \subsubsection{(iv)}
% We can find the gradient our relaxed objective using \eqref{lem:grad-fdelta}.
% For interior points,
% \begin{align}
%     \nabla g_\delta (\v t, \v \xi) 
%     = \nabla (\frac{1}{2^n} \sum_{i = 1}^{2^n} f_\delta (\v t, \v \xi_i))
%     &= \frac{1}{2^n} \sum_{i = 1}^{2^n} \nabla f_\delta (\v t, \v \xi_i) \\
%     &= \frac{1}{2^n} \sum_{i = 1}^{2^n} 2 \delta \frac{(\Pi_{\v t, \v \xi_i}^{-1} \v 1)^2 }{\v t^3},
% \end{align}
% where 
% \begin{align}
%     \Pi_{\v t, \v \xi_i} 
%     :=
%     \Sigma^{(\v \xi_i)} + \delta ( I -  T_{\v t}^{-2}).
% \end{align}
% Also, note that in lemma 1, $\Pi_{\v t, \v \xi_i} 
%     :=
%     \Sigma^{(\v \xi_i)} + \delta ( T_{\v t}^{-2} - I).$

% Note that, all the elements of gradient $\nabla g_\delta (\v t, \v \xi)$ are non-negative since each $\nabla f_\delta (\v t, \v \xi)$ are non-negative.
% Hence, objective function $g_\delta (\v t, \v \xi)$ is non-decreasing along $t_j$.
% Hence, simplex $\cS_k$ contains an optimal solution of boolean relaxation.


% % Since in lemma 3, we showed that every element is non-negative, its summation will also be non-negative. (Note that, in the paper, function had negative sign in front of it.) And hence, $g_\delta$ will also be non-decreasing, so optimal solution is contained by the simplex $\cS_k$.


% \subsubsection{(v)}
% (Refer to Theorem 3 in Scalable-Gradient)
% We showed that every $f_\delta (\v t , \v \xi)$ is strictly convex over $[0, 1]^p$ for $\delta \geq \eta_1$ where $\eta_1$ is the largest eigenvalue of $\Sigma^{(\v \xi_i)}$.
% Hence, our objective function $g_\delta (\v t, \v \xi)$ would be also strictly convex because it is a finite sum of $2^n$ strictly convex functions.


% Explicitly show that Hessian is negative definite...?

% First, we propose a scaling of our problem below.
% Note that, 
% \begin{align}
%     \v 1^{\top} \Sigma_{[\v s], \v \xi}^{-1} \v 1 
%     = 
%     \v w_{[\v s]}^{\top} R_{[\v s], \v \xi}^{-1} \v w_{[\v s]}
% \end{align}
% $R$ be a scaled matrix (somewhat similar to a correlation matrix) where $R_{i,j} = $


% Now we expect to prove that Boolean relaxation of \eqref{eqn:sparse-portfolio-master} is is indeed a boolean relaxation of \eqref{eq:cssp-core-expected} thanks to the fact that expectation operator is a linear operator.

\subsection{Scaling - CSSP}
Scaling our $\Sigma$ to $R$ (correlation matrix like) as scalable-gradient paper has done...

This is valid because all diagonal elements of $\Sigma$ is not 0, thanks to our assumption of $A$ being positive definite (Thus, diagonal elements are all positive) and all elements of $\frac{\v 1}{\v b}$ are non-zero, as we have shown earlier.

\subsection{Gradient - CSSP}

\subsection{Algorithm - CSSP}
For convenience, we reformulate \eqref{eq:cssp-hadamard-reform} and \eqref{eq:cssp-boolean-relax} into a maximisation problem below,
\begin{align}
\label{eq:cssp-hadamard-reform}
&\min_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
-\ee_{\v \xi}\!\Big[\,
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
(A_{[\v s]})^{-1}
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})
\,\Big] \\
= 
&\min_{\v s \in \{0, 1\}^p:\,|\v s|\le k} \;
\ee_{\v \xi}\!\Big[\,
-(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})^{\top}
(A_{[\v s]})^{-1}
(\v 1 \odot \v b^{(\v \xi_i)}_{[\v s]})
\,\Big]
\end{align} 
, and the boolean relaxation as,
\begin{align}
\label{eq:cssp-boolean-relax}
\minimize_{\v t \in \cC} \;
 z_\delta(\v t) 
=
\minimize_{\v t \in \cC} \;
\ee_{\v \xi}\!\Big[\,
h_\delta (\v t, \v \xi)
\,\Big]
\end{align},
where $h_\delta (\v t, \v \xi) = f_\delta (\v t, \v \xi)-[b^{(\v \xi)} \odot \v t]^{\top} (\wt A_{\v t})^{-1} [b^{(\v \xi)} \odot \v t] $,
and $\wt A_{\v t} = T_{\v t} A T_{\v t} + \delta (I - T_{\v t}^2 ).$

And, gradient of $h$ will be 
\begin{align*}
\nabla h_\delta (\v t, \v \xi) = - \nabla f_\delta (\v t, \v \xi).
\end{align*}

\begin{algorithm}
\caption{\texttt{FW-Homotopy}$(A, k, \alpha, n, m)$}
\label{alg:alg}
\begin{algorithmic}[1]
% \State Create $m$ random Rademacher vectors $\v \xi_i$ for $i = 1, \dots ,m$.
\State Compute the largest and smallest eigenvalues $\eta_1$ and $\eta_p$ of $A$
\State $\tau \leftarrow 10^{-4}$ \Comment{Tolerance for termination}
\State $\varepsilon \leftarrow 0.1 (k/p)$ 
\State $\delta_0 \leftarrow 3\eta_p\varepsilon^2/(1 + 3\varepsilon^2)$ 
\State $r \leftarrow (\eta_1/\delta_0)^{1/(n-1)}$
\State $\v t \leftarrow (k/p) \v 1$
\State $\ell \leftarrow 1$
\Repeat
    \State $\delta \leftarrow \delta_0 r^{\ell}$
    \State Create $m$ random Rademacher vectors $\v \xi_i$ for $i = 1, \dots ,m$
    \State Compute the gradient $\nabla z_{\delta}(\v t)$, average of $q$ gradients $\nabla z_{\delta}(\v t, \v \xi_q)$
    \State Let $\v s \in \{0,1\}^p$ have ones at the positions of the $k$ \textit{smallest} components of $\nabla z_{\delta}(\v t)$
    \State $\v t \leftarrow (1-\alpha)\v t + \alpha \v s$
    \State $\ell \leftarrow \ell + 1$
    \If{$\displaystyle \min_{j=1,\dots,p}\min\{t_j,\,1-t_j\}\le \tau$}
        \State Set $\delta \leftarrow \eta_1$ and compute $\v z \leftarrow \nabla h_{\delta}(\v s)$ 
        \If{$\displaystyle \max_{j:\,s_j=1} z_j \le \min_{i:\,s_i=0} z_i$} \Comment{KKT-Certification at $\delta=\eta_1$}
            \State \Return $\v s$
        \EndIf
    \EndIf
\Until{$\ell > n$}
\State \Return $\v s$
\end{algorithmic}
\end{algorithm}
\subsection{Implementation Challenges and Numerical Stability}

During the development and verification of the Frank-Wolfe Homotopy algorithm, several numerical challenges were identified. 
% These issues primarily stemmed from the ill-conditioned nature of the objective function and the stochastic properties of the gradient estimation. We categorize these challenges into three main areas: floating-point instability during verification, matrix singularity in real-world datasets, and stochastic variance in low-dimensional selections.

\subsubsection{Numerical Gradient Verification at High Condition Numbers}
\textbf{Challenge:}
In the unit testing phase, we compared the analytical gradient $\nabla f(t)$ against a numerical approximation using the central difference method:
\begin{equation}
    \frac{\partial f}{\partial t_i} \approx \frac{f(t + h e_i) - f(t - h e_i)}{2h}
\end{equation}
While this verification held for well-conditioned matrices, it failed consistently for matrices with condition numbers $\kappa(A) \geq 10^8$.

\textbf{Diagnosis:}
The failure was attributed to \textit{catastrophic cancellation}. When $\kappa(A)$ is large, the function $f(t)$ becomes extremely steep. The floating-point representation of $f(t+h)$ and $f(t-h)$ are nearly identical in magnitude but contain significant noise in their lower bits. For example, when $f(t +h) = 10000.0001$ and $f(t - h) = 10000.0000$.
Subtracting these values results in a loss of precision that dominates the finite difference approximation, diverging from the analytical solution.

\textbf{Resolution:}
We established an adaptive testing protocol. For cases where $\kappa(A) > 10^7$, we relaxed the verification tolerance and increased the step size $h$ from $10^{-7}$ to $10^{-5}$ to step over the floating-point noise floor.

% \subsubsection{Singularity in Real-World Covariance Matrices}
% \textbf{Challenge:}
% When benchmarking on the SECOM dataset, the algorithm occasionally returned negative objective values or terminated with linear algebra errors. This is mathematically impossible for the trace of the inverse of a positive semi-definite (PSD) matrix.

% \textbf{Diagnosis:}
% Real-world datasets often contain highly correlated or duplicate features. During the subset selection process, the submatrix $A_{SS}$ (corresponding to the selected features) became nearly singular ($\det(A_{SS}) \approx 0$). Consequently, the standard inversion operation \texttt{np.linalg.inv} became numerically unstable, producing garbage values.

% \textbf{Resolution:}
% We implemented Tikhonov regularization (diagonal loading) during the objective calculation. We replaced the direct inversion with:
% \begin{equation}
%     A_{SS}^{-1} \approx (A_{SS} + \epsilon I)^{-1}
% \end{equation}
% where $\epsilon = 10^{-6}$. This ensures the matrix remains strictly positive definite without significantly altering the objective value for valid selections.

\subsubsection{Stochastic Variance in Low-$k$ Regimes}
\textbf{Challenge:}
While the algorithm outperformed the Greedy heuristic for $k \geq 50$, it initially underperformed (reaching only 60\% of the optimal objective) for small subset sizes (e.g., $k=10$).

\textbf{Diagnosis:}
The gradient of the expected objective $g(t) = \mathbb{E}_{\xi}[\dots]$ is estimated via Monte Carlo sampling. For small $k$, the solution space is sparse. With a low number of samples ($N_{mc} = 30$), the stochastic gradient estimator exhibited high variance, failing to identify the "sharp peaks" of the optimal features among the noise.

\textbf{Resolution:}
We adopted an adaptive sampling strategy based on the problem size. For small $k$ ($k < 20$), we increased the Monte Carlo samples to $N_{mc} = 300$. This reduced the variance of the estimator, allowing the algorithm to recover $>95\%$ accuracy relative to the Greedy baseline.
Create $m$ Rademacher vectors at the start of the run. Then, we run \textit{modified} FW-Homotopy algorithm on all $m$ sparse QP problem, and average out.

\textbf{Improvements:}
Since our algorithm's performance heavily depends on $N_{mc}$, we can further investigate if there is a more general, unified approach in terms of $N_{mc}$'s selection


Our gradient is a form of sum of $2^n$ terms.
Each term is fairly easy to calculate because note that $\Pi_{\v t}^{-1}$ is a constant in terms of $\v \xi$, so for each term, the only part changes is $\v b^{(\v \xi_i)}$.
Hence, rather than trying all the $2^n$ terms to find a gradient, we sample some number of gradients to calculate to \textit{estimate} the gradient of $g$.




However, we don't actually have to calculate all $2^n$ terms.
Rather, we sample $0< m < 2^n$ terms such that our sampled gradient is close enough to the actual gradient...
(Set a threshold...and actually verify if this claim is true?)

Then, we can use the same algorithm as given...



\section{Boolean relaxation}
We now provide a Boolean relaxation of \eqref{eqn:sparse-portfolio-master} as an auxiliary continuous function on $[0, 1]^p$, controlled by a tuning parameter $\delta > 0$.  \sbmc{[This stuff is from Moka et al (2025) on portfolio optimization]}.
To simplify the notation, define 
\begin{align}
T_{\v t} = \diag(\v t) \quad 
\text{and} \quad 
\wt \Sigma_{\v t} = T_{\v t}\Sigma T_{\v t} + \delta (I - T_{\v t}^2).
\label{eqn:defn-Sigma}
\end{align}
Then, our proposed Boolean relaxation of \eqref{eqn:sparse-portfolio-master} is given by
\begin{align}
    \label{eqn:opt-br2}
    \minimize_{\v t \in \cC_k} f_\delta(\v t), \quad \text{where}\,\,\,\,
f_\delta(\v t) = - \v t^\top {\wt \Sigma}_{\v t}^{-1} \v t,
\end{align}
and for each $k$ the constraint set $\cC_k$ is a polytope defined as
\begin{align}
\cC_k = \{ \v t \in [0, 1]^p : \v t^\top \v 1 \leq k\}.
\label{eqn:defn-polytope}
\end{align} 

The following result, Theorem~\ref{thm:relaxation-properties}, shows why \eqref{eqn:opt-br2} is a relaxation of the target problem \eqref{eqn:opt-bc3}. It shows that $f_\delta(\v t)$ is continuous on the hypercube $[0, 1]^p$ and its shape can be controlled by the auxiliary parameter $\delta$ while keeping the values of $f_\delta(\v t)$ fixed---independent of $\delta$---at all the (binary) corners $\v s \in \{ 0,1\}^p$. In addition, (iii) shows that $f_\delta(\v t)$ increases with $\delta$ for any fixed interior point $\v t$, while (iv) shows that the optimum of \eqref{eqn:opt-br2} is on a simplex.
\begin{theorem}[Theorems 2 \& 3 of \cite{moka2025scalable}]
    \label{thm:relaxation-properties}
    The following hold:
    \begin{itemize}
        \item[(i)] The objective function $f_\delta(\v t)$ in \eqref{eqn:opt-br2} is continuous on $[0,1]^p$.
        \item[(ii)] For every binary vector $\v s \in \{0, 1\}^p$ (i.e., a corner point on the hypercube $[0, 1]^p$),
        \[
        f_\delta(\v s) = - \v 1^\top  \Sigma_{[\v s]}^{-1} \v 1, \quad \text{for all}\,\, \delta > 0.
        \]
        \item[(iii)] For every fixed $\v t \in (0, 1)^p$, $f_\delta(\v t)$ is monotonically increasing in $\delta > 0$.
        \item[(iv)] For any $k =1, \dots, p$ and $\delta > 0$, 
        \[
        \minimize_{\v t \in \cC_k} f_\delta(\v t) = \minimize_{\v t \in \cS_k} f_\delta(\v t),
        \]
        here, the simplex $\cS_k = \{\v t \in [0, 1]^p : \v t^\top \v 1 =k \}$ corresponds to the polytope $\cC_k$ given in \eqref{eqn:defn-polytope}.
        \item[(v)]  Let $\eta_1$ be the largest eigenvalue of $\Sigma$. Then, 
        $f_\delta(\v t)$ strictly concave over $[0,1]^p$ for $\delta \geq \eta_1$.
    \end{itemize}
\end{theorem}

\subsection{Gradient}
We first derive a convenient closed--form expression for the gradient of $f_\delta$.


\begin{lemma}[Gradient of the relaxed objective]
\label{lem:grad-fdelta}
For each $\v t \in [0,1]^p$ and define
\(
\v x := \wt\Sigma_{\v t}^{-1}\v t
\)
and
\(
\v z := \Sigma (\v t \odot \v x).
\)
Then $f_\delta$ is differentiable at $\v t$ and
\begin{equation}
\label{eq:grad-vector}
\nabla f_\delta(\v t)
=
-2\,\v x
\;+\;
2\,\v x \odot \v z
\;-\;
2\delta\,\v t \odot \v x \odot \v x .
\end{equation}
Moreover, with
\(
\Pi_{\v t}
:= \Sigma + \delta(T_{\v t}^{-2} - I),
\)
on the interior points, the gradient admits the equivalent form
\begin{align}
\nabla f_\delta(\v t)
= -2\delta\,\frac{(\Pi_{\v t}^{-1}\v 1)^2}{\v t^3},
\qquad \v t\in(0,1)^p.
\label{eq:grad-vector-interior}
\end{align}
where all operations between vectors are elementwise.
\end{lemma}

\section{Algorithms for KKT-Minimal Points}
In this section, we develop several continuous optimization algorithms for the relaxed problem
\eqref{eqn:opt-br2} over the simplex $\cS_k$.  The algorithms operate by updating the regularization parameter $\delta$ during the
iterations and are designed to converge to KKT binary corner of $\cS_k$ at $\delta=\eta_1$,
yielding a candidate solution for the
original sparse portfolio problem.

\subsection{Frank-Wolfe Homotomy Method}
Algorithm~\ref{alg:alg} is a variant of the standard Frank-Wolfe algorithm, similar to the \texttt{Grid-FW} of \cite{moka2025scalable}. This algorithm is coupled with a continuation scheme in the regularization parameter
$\delta$. 

\begin{algorithm}
\caption{\texttt{FW-Homotopy}$(\Sigma, k, \alpha, n)$}
\label{alg:alg}
\begin{algorithmic}[1]
\State Compute the largest and smallest eigenvalues $\eta_1$ and $\eta_p$ of $\Sigma$
\State $\tau \leftarrow 10^{-4}$ \Comment{Tolerance for termination}
\State $\varepsilon \leftarrow 0.1 (k/p)$ 
\State $\delta_0 \leftarrow 3\eta_p\varepsilon^2/(1 + 3\varepsilon^2)$ 
\State $r \leftarrow (\eta_1/\delta_0)^{1/(n-1)}$
\State $\v t \leftarrow (k/p) \v 1$
\State $\ell \leftarrow 1$
\Repeat
    \State $\delta \leftarrow \delta_0 r^{\ell}$
    \State Compute the gradient $\nabla f_{\delta}(\v t)$
    \State Let $\v s \in \{0,1\}^p$ have ones at the positions of the $k$ smallest components of $\nabla f_{\delta}(\v t)$
    \State $\v t \leftarrow (1-\alpha)\v t + \alpha \v s$
    \State $\ell \leftarrow \ell + 1$
    \If{$\displaystyle \min_{j=1,\dots,p}\min\{t_j,\,1-t_j\}\le \tau$}
        \State Set $\delta \leftarrow \eta_1$ and compute $\v g \leftarrow \nabla f_{\delta}(\v s)$ 
        \If{$\displaystyle \max_{j:\,s_j=1} g_j \le \min_{i:\,s_i=0} g_i$} \Comment{KKT-Certification at $\delta=\eta_1$}
            \State \Return $\v s$
        \EndIf
    \EndIf
\Until{$\ell > n$}
\State \Return $\v s$
\end{algorithmic}
\end{algorithm}
\sbmc{[Explain the algorithm.]} 

\begin{appendix}
\section{Proofs}
\begin{proof}[Proof of Lemma~\ref{lem:grad-fdelta}]
For $\v t\in[0,1]^p$, since $\wt\Sigma_{\v t}$ invertible,
using the identity
\(
\partial A^{-1} = -A^{-1}(\partial A)A^{-1}
\)
for matrix differentials and the product rule, one obtains
\[
\frac{\partial f_\delta}{\partial t_i}
=
-2x_i
\;+\;
2x_i z_i
\;-\;
2\delta\, t_i x_i^2,
\]
where 
$\v x = \wt\Sigma_{\v t}^{-1}\v t$
and 
$\v z = \Sigma(\v t\odot\v x)$.
Collecting the components yields \eqref{eq:grad-vector}, which is well-defined for all such $\v t\in[0,1]^p$ (no division by $t_i$ is involved).

For the second expression, restrict to interior points $\v t\in(0,1)^p$, so that $T_{\v t}$ is invertible. Then
\[
\wt\Sigma_{\v t}
= T_{\v t}\Pi_{\v t}T_{\v t},
\qquad
\text{with}\,\,\,\Pi_{\v t}
:= \Sigma + \delta(T_{\v t}^{-2}-I),
\]
and hence
\[
\wt\Sigma_{\v t}^{-1}\v t
= T_{\v t}^{-1}\Pi_{\v t}^{-1}T_{\v t}^{-1}\v t
= T_{\v t}^{-1}\Pi_{\v t}^{-1}\v 1.
\]
Thus
\[
x_i = \frac{\bigl(\Pi_{\v t}^{-1}\v 1\bigr)_i}{t_i},
\qquad
t_i>0.
\]
Substituting this representation of $x_i$ (and the corresponding expression for $z_i$) into
\eqref{eq:grad-vector}, the terms $-2x_i+2x_i z_i$ cancel, yielding
\[
\frac{\partial f_\delta}{\partial t_i}
= -2\delta\, t_i x_i^2
= -2\delta\,\frac{\bigl(\Pi_{\v t}^{-1}\v 1\bigr)_i^2}{t_i^3},
\quad i=1,\dots,p.
\]
Writing this in vector form gives the stated alternative gradient expression for $\v t\in(0,1)^p$.
\end{proof}
\end{appendix}
\bibliographystyle{apalike}
\bibliography{Refs}
\end{document}

