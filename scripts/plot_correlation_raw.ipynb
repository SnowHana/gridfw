{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109a0f85",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "- Compare correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dbda61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics... (This may take a moment for large datasets)\n",
      "Processing synthetic_high_corr...\n",
      "  > Loading dataset: synthetic_high_corr\n",
      "    Generating Synthetic Data: N=2000, p=500, Blocks=1, Corr=0.95\n",
      "    Computed Correlation Matrix A: (500, 500)\n",
      "Processing synthetic_toeplitz...\n",
      "  > Loading dataset: synthetic_toeplitz\n",
      "    Generating Toeplitz Trap: p=500, rho=0.9\n",
      "Processing residential...\n",
      "  > Loading dataset: residential\n",
      "    Source: /Users/nautilus/gridfw/data/residential.xlsx\n",
      "    Raw Data Shape: 372 rows x 103 features\n",
      "    Computed Correlation Matrix A: (103, 103)\n",
      "Processing secom...\n",
      "  > Loading dataset: secom\n",
      "    Source: /Users/nautilus/gridfw/data/secom.data\n",
      "    [secom Cleaning] Dropped 112 constant columns.\n",
      "    Raw Data Shape: 1567 rows x 478 features\n",
      "    Computed Correlation Matrix A: (478, 478)\n",
      "Processing arrhythmia...\n",
      "  > Loading dataset: arrhythmia\n",
      "    Source: /Users/nautilus/gridfw/data/arrhythmia.data\n",
      "    [arrhythmia Cleaning] Dropped 17 constant columns.\n",
      "    Raw Data Shape: 452 rows x 262 features\n",
      "    Computed Correlation Matrix A: (262, 262)\n",
      "Processing mnist...\n",
      "  > Loading dataset: mnist\n",
      "    Fetching MNIST from OpenML (this may take a moment)...\n",
      "    [mnist Cleaning] Dropped 145 constant columns.\n",
      "    Raw Data Shape: 2000 rows x 639 features\n",
      "    Computed Correlation Matrix A: (639, 639)\n",
      "Processing madelon...\n",
      "  > Loading dataset: madelon\n",
      "    Fetching Madelon from OpenML...\n",
      "    Raw Data Shape: 2600 rows x 500 features\n",
      "    Computed Correlation Matrix A: (500, 500)\n",
      "Processing myocardial...\n",
      "  > Loading dataset: myocardial\n",
      "    Source: UCI Repo ID 579\n",
      "    Raw Data Shape: 1700 rows x 111 features\n",
      "    Computed Correlation Matrix A: (111, 111)\n",
      "\n",
      "============================================================\n",
      "DATASET CHARACTERISTICS SUMMARY\n",
      "============================================================\n",
      "            Dataset  Samples (n)  Features (p) Condition No. (κ) Avg. Corr Max. Corr\n",
      "Synthetic_high_corr          500           500          2.26e+13     0.087     0.261\n",
      " Synthetic_toeplitz          500           500          2.27e+13     0.130     0.993\n",
      "        Residential          103           103          8.51e+12     0.759     1.000\n",
      "              Secom          478           478          2.50e+13     0.104     1.000\n",
      "         Arrhythmia          262           262          1.13e+13     0.191     1.000\n",
      "              Mnist          639           639          5.65e+13     0.173     1.000\n",
      "            Madelon          500           500          4.81e+12     0.031     1.000\n",
      "         Myocardial          111           111          8.64e+11     0.069     0.999\n",
      "\n",
      "============================================================\n",
      "LATEX CODE (Copy to Thesis)\n",
      "============================================================\n",
      "\\begin{table}\n",
      "\\caption{Statistical Characteristics of Benchmark Datasets}\n",
      "\\label{tab:datasets_stats}\n",
      "\\begin{tabular}{lrrlll}\n",
      "\\toprule\n",
      "Dataset & Samples (n) & Features (p) & Condition No. (κ) & Avg. Corr & Max. Corr \\\\\n",
      "\\midrule\n",
      "Synthetic_high_corr & 500 & 500 & 2.26e+13 & 0.087 & 0.261 \\\\\n",
      "Synthetic_toeplitz & 500 & 500 & 2.27e+13 & 0.130 & 0.993 \\\\\n",
      "Residential & 103 & 103 & 8.51e+12 & 0.759 & 1.000 \\\\\n",
      "Secom & 478 & 478 & 2.50e+13 & 0.104 & 1.000 \\\\\n",
      "Arrhythmia & 262 & 262 & 1.13e+13 & 0.191 & 1.000 \\\\\n",
      "Mnist & 639 & 639 & 5.65e+13 & 0.173 & 1.000 \\\\\n",
      "Madelon & 500 & 500 & 4.81e+12 & 0.031 & 1.000 \\\\\n",
      "Myocardial & 111 & 111 & 8.64e+11 & 0.069 & 0.999 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from grad_fw.data_loader import DatasetLoader\n",
    "\n",
    "def get_dataset_metrics(dataset_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes key characteristics for a single dataset:\n",
    "    - Dimensions (n, p)\n",
    "    - Condition Number (Kappa)\n",
    "    - Average & Maximum Feature Correlation\n",
    "    \"\"\"\n",
    "    loader = DatasetLoader()\n",
    "    \n",
    "    try:\n",
    "        # Load Data\n",
    "        A, _  = loader.load(dataset_name, **kwargs)\n",
    "        \n",
    "        if A is None:\n",
    "            return None\n",
    "\n",
    "        # 1. Basic Dimensions\n",
    "        n, p = A.shape\n",
    "        \n",
    "        # 2. Condition Number Calculation (Landscape Flatness)\n",
    "        # Normalize first to ensure scale-invariance\n",
    "        X_centered = A - np.mean(A, axis=0)\n",
    "        std = np.std(X_centered, axis=0)\n",
    "        std[std == 0] = 1.0 # Prevent div by zero\n",
    "        X_scaled = X_centered / std\n",
    "        \n",
    "        # Compute eigenvalues of Gram Matrix (X.T @ X)\n",
    "        # Note: For very large p, this might be slow. If p > 5000, consider randomized SVD.\n",
    "        # But for your thesis datasets, this is fine.\n",
    "        Gram = X_scaled.T @ X_scaled\n",
    "        evals = np.linalg.eigvalsh(Gram)\n",
    "        \n",
    "        lambda_max = evals[-1]\n",
    "        lambda_min = max(evals[0], 1e-9) # Avoid 0\n",
    "        cond_number = lambda_max / lambda_min\n",
    "\n",
    "        # 3. Correlation Statistics (Redundancy)\n",
    "        df_temp = pd.DataFrame(A)\n",
    "        corr_matrix = df_temp.corr().abs()\n",
    "        \n",
    "        # Exclude diagonal (self-correlation is always 1)\n",
    "        mask = np.ones(corr_matrix.shape, dtype=bool)\n",
    "        np.fill_diagonal(mask, 0)\n",
    "        off_diag = corr_matrix.values[mask]\n",
    "        \n",
    "        avg_corr = np.mean(off_diag) if len(off_diag) > 0 else 0.0\n",
    "        max_corr = np.max(off_diag) if len(off_diag) > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            \"Dataset\": dataset_name.capitalize(),\n",
    "            \"Samples (n)\": n,\n",
    "            \"Features (p)\": p,\n",
    "            \"Condition No. (κ)\": cond_number,\n",
    "            \"Avg. Corr\": avg_corr,\n",
    "            \"Max. Corr\": max_corr\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_characteristics_table():\n",
    "    # List of your thesis datasets\n",
    "    datasets = [\n",
    "        (\"synthetic_high_corr\", {\"n_blocks\": 1, \"correlation_strength\": 0.95}),\n",
    "        (\"synthetic_toeplitz\", {}),\n",
    "        (\"residential\", {}),\n",
    "        (\"secom\", {}),\n",
    "        (\"arrhythmia\", {}),\n",
    "        (\"mnist\", {}),\n",
    "        (\"madelon\", {}),\n",
    "        (\"myocardial\", {})\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    print(\"Computing metrics... (This may take a moment for large datasets)\")\n",
    "    \n",
    "    for name, kwargs in datasets:\n",
    "        print(f\"Processing {name}...\")\n",
    "        metrics = get_dataset_metrics(name, **kwargs)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # --- Formatting for Display ---\n",
    "    # Scientific notation for Condition Number (it can get HUGE)\n",
    "    pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "    \n",
    "    # We create a display copy to make it pretty\n",
    "    df_display = df.copy()\n",
    "    df_display[\"Condition No. (κ)\"] = df_display[\"Condition No. (κ)\"].map(lambda x: f\"{x:.2e}\")\n",
    "    df_display[\"Avg. Corr\"] = df_display[\"Avg. Corr\"].map(lambda x: f\"{x:.3f}\")\n",
    "    df_display[\"Max. Corr\"] = df_display[\"Max. Corr\"].map(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET CHARACTERISTICS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    # Generate LaTeX code for your thesis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LATEX CODE (Copy to Thesis)\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_display.to_latex(index=False, caption=\"Statistical Characteristics of Benchmark Datasets\", label=\"tab:datasets_stats\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_characteristics_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d515f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: synthetic_high_corr\n",
      "============================================================\n",
      "  > Loading dataset: synthetic_high_corr\n",
      "    Generating Synthetic Data: N=2000, p=500, Blocks=1, Corr=0.95\n",
      "    Computed Correlation Matrix A: (500, 500)\n",
      "\n",
      "[1] DATASET DIMENSIONS\n",
      "    Rows: 2000\n",
      "    Columns (Features): 500\n",
      "\n",
      "[!!] Condition number : 845.3990247539451\n",
      "\n",
      "[2] COMPUTING COLUMN CORRELATIONS...\n",
      "\n",
      "[3] REDUNDANCY STATISTICS\n",
      "    Average Feature Correlation: 0.9973\n",
      "    Maximum Feature Correlation: 0.9976\n",
      "\n",
      "[4] SAMPLE CORRELATION BLOCK (First 5 Features)\n",
      "    This shows how similar the first few features are to each other:\n",
      "        0       1       2       3       4\n",
      "0  1.0000  0.9974  0.9973  0.9973  0.9975\n",
      "1  0.9974  1.0000  0.9973  0.9974  0.9974\n",
      "2  0.9973  0.9973  1.0000  0.9973  0.9974\n",
      "3  0.9973  0.9974  0.9973  1.0000  0.9973\n",
      "4  0.9975  0.9974  0.9974  0.9973  1.0000\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: residential\n",
      "============================================================\n",
      "  > Loading dataset: residential\n",
      "    Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00437/Residential-Building-Data-Set.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nautilus/gridfw/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'archive.ics.uci.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Raw Data Shape: 372 rows x 103 features\n",
      "    Computed Correlation Matrix A: (103, 103)\n",
      "\n",
      "[1] DATASET DIMENSIONS\n",
      "    Rows: 372\n",
      "    Columns (Features): 103\n",
      "\n",
      "[!!] Condition number : 1.4223510128590456e+17\n",
      "\n",
      "[2] COMPUTING COLUMN CORRELATIONS...\n",
      "\n",
      "[3] REDUNDANCY STATISTICS\n",
      "    Average Feature Correlation: 0.5868\n",
      "    Maximum Feature Correlation: 1.0000\n",
      "\n",
      "[4] SAMPLE CORRELATION BLOCK (First 5 Features)\n",
      "    This shows how similar the first few features are to each other:\n",
      "        0       1       2       3       4\n",
      "0  1.0000 -0.2295 -0.1381 -0.2869 -0.3136\n",
      "1 -0.2295  1.0000  0.9465  0.7695  0.2266\n",
      "2 -0.1381  0.9465  1.0000  0.6392  0.1544\n",
      "3 -0.2869  0.7695  0.6392  1.0000  0.5823\n",
      "4 -0.3136  0.2266  0.1544  0.5823  1.0000\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: secom\n",
      "============================================================\n",
      "  > Loading dataset: secom\n",
      "    Source: https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nautilus/gridfw/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'archive.ics.uci.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 2. Test Real Data (Residential)\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# This likely has lower average correlation, which explains why FW works differently here\u001b[39;00m\n\u001b[32m     93\u001b[39m analyze_feature_correlation(\u001b[33m\"\u001b[39m\u001b[33mresidential\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43manalyze_feature_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msecom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m analyze_feature_correlation(\u001b[33m\"\u001b[39m\u001b[33marrhythmia\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m analyze_feature_correlation(\u001b[33m\"\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36manalyze_feature_correlation\u001b[39m\u001b[34m(dataset_name, n_samples, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 1. Load Data\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Note: We rely on the loader to return 'A' (matrix) and 'X_norm' (the actual data).\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# We only care about X_norm here.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     _, X_raw = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/src/grad_fw/data_loader.py:94\u001b[39m, in \u001b[36mDatasetLoader.load\u001b[39m\u001b[34m(self, name_or_url, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m url = DATASETS_URL.get(key, name_or_url)\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m response.raise_for_status()\n\u001b[32m     96\u001b[39m content = io.BytesIO(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/urllib3/response.py:1246\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1231\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1243\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[32m   1249\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp)\n\u001b[32m   1250\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m\n\u001b[32m   1251\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1252\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/urllib3/response.py:1414\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1412\u001b[39m     chunk = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1416\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gridfw/.venv/lib/python3.14/site-packages/urllib3/response.py:1329\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1330\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/lib/python3.14/socket.py:725\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    727\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/lib/python3.14/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.0/lib/python3.14/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from grad_fw.data_loader import DatasetLoader\n",
    "\n",
    "def analyze_feature_correlation(dataset_name, n_samples=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a dataset and computes the pairwise Pearson correlation between its columns (features).\n",
    "    Prints statistics to prove feature redundancy.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset (e.g., 'synthetic_high_corr', 'residential').\n",
    "        n_samples (int, optional): Limit rows for speed (e.g., 2000). None uses all data.\n",
    "        **kwargs: Extra arguments for synthetic generation (e.g., n_blocks=1).\n",
    "    \"\"\"\n",
    "    loader = DatasetLoader()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    # Note: We rely on the loader to return 'A' (matrix) and 'X_norm' (the actual data).\n",
    "    # We only care about X_norm here.\n",
    "    try:\n",
    "        _, X_raw = loader.load(dataset_name, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    if X_raw is None:\n",
    "        print(\"Dataset not found or failed to load.\")\n",
    "        return\n",
    "\n",
    "    # Subsample rows if requested (does not affect column correlation logic, just speed)\n",
    "    if n_samples and X_raw.shape[0] > n_samples:\n",
    "        X_raw = X_raw[:n_samples]\n",
    "\n",
    "    # 2. Convert to Pandas for easy Correlation calculation\n",
    "    df = pd.DataFrame(X_raw)\n",
    "    n_rows, n_cols = df.shape\n",
    "    \n",
    "    print(f\"\\n[1] DATASET DIMENSIONS\")\n",
    "    print(f\"    Rows: {n_rows}\")\n",
    "    print(f\"    Columns (Features): {n_cols}\")\n",
    "\n",
    "    # Get condition number\n",
    "    print(f\"\\n[!!] Condition number : {np.linalg.cond(X_raw)}\")\n",
    "    \n",
    "    # 3. Compute Full Correlation Matrix (Feature vs Feature)\n",
    "    print(\"\\n[2] COMPUTING COLUMN CORRELATIONS...\")\n",
    "    corr_matrix = df.corr().abs() # We use absolute value to treat -0.99 same as 0.99\n",
    "\n",
    "    # 4. Extract Key Statistics\n",
    "    # Mask the diagonal (which is always 1.0) to find true cross-correlations\n",
    "    mask = np.ones(corr_matrix.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, 0)\n",
    "    off_diag_corrs = corr_matrix.values[mask]\n",
    "\n",
    "    avg_corr = np.mean(off_diag_corrs)\n",
    "    max_corr = np.max(off_diag_corrs)\n",
    "    \n",
    "    # 5. Proof: Specific Examples\n",
    "    print(f\"\\n[3] REDUNDANCY STATISTICS\")\n",
    "    print(f\"    Average Feature Correlation: {avg_corr:.4f}\")\n",
    "    print(f\"    Maximum Feature Correlation: {max_corr:.4f}\")\n",
    "\n",
    "    # Show a specific high-correlation pair\n",
    "    # Find indices of max correlation\n",
    "    # We flatten the matrix, find argmax, then unravel index\n",
    "    # (Using a simpler loop for clarity in output)\n",
    "    \n",
    "    print(f\"\\n[4] SAMPLE CORRELATION BLOCK (First 5 Features)\")\n",
    "    print(\"    This shows how similar the first few features are to each other:\")\n",
    "    print(df.iloc[:, :5].corr().round(4))\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION EXAMPLES\n",
    "# ==========================================\n",
    "\n",
    "# 1. Test your \"Dense\" Synthetic Data (The one for the email)\n",
    "# This proves the \"All columns are clones\" argument\n",
    "analyze_feature_correlation(\n",
    "    \"synthetic_high_corr\", \n",
    "    n_blocks=1, \n",
    "    correlation_strength=0.95\n",
    ")\n",
    "\n",
    "# 2. Test Real Data (Residential)\n",
    "# This likely has lower average correlation, which explains why FW works differently here\n",
    "analyze_feature_correlation(\"residential\")\n",
    "analyze_feature_correlation(\"secom\")\n",
    "analyze_feature_correlation(\"arrhythmia\")\n",
    "analyze_feature_correlation(\"mnist\")\n",
    "analyze_feature_correlation(\"madelon\")\n",
    "\n",
    "# 3. Test Myocardial\n",
    "# analyze_feature_correlation(\"myocardial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
